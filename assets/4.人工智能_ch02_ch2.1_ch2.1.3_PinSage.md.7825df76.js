import{_ as e,o as l,c as p,j as s,a as n,U as a}from"./chunks/framework.489e5108.js";const $n=JSON.parse('{"title":"Graph Convolutional Neural Networks for Web-Scale Recommender Systems","description":"","frontmatter":{},"headers":[],"relativePath":"4.人工智能/ch02/ch2.1/ch2.1.3/PinSage.md","filePath":"4.人工智能/ch02/ch2.1/ch2.1.3/PinSage.md","lastUpdated":1696176798000}'),o={name:"4.人工智能/ch02/ch2.1/ch2.1.3/PinSage.md"},t=a('<h1 id="graph-convolutional-neural-networks-for-web-scale-recommender-systems" tabindex="-1">Graph Convolutional Neural Networks for Web-Scale Recommender Systems <a class="header-anchor" href="#graph-convolutional-neural-networks-for-web-scale-recommender-systems" aria-label="Permalink to &quot;Graph Convolutional Neural Networks for Web-Scale Recommender Systems&quot;">​</a></h1><p>该论文是斯坦福大学和 Pinterest 公司与 2018 年联合发表与 KDD 上的一篇关于 GCN 成功应用于工业级推荐系统的工作。该论文提到的 PinSage 模型，是在 GraphSAGE 的理论基础进行了更改，以适用于实际的工业场景。下面将简单介绍一下 GraphSAGE 的原理，以及 Pinsage 的核心和细节。</p><h2 id="graphsage原理" tabindex="-1">GraphSAGE 原理 <a class="header-anchor" href="#graphsage原理" aria-label="Permalink to &quot;GraphSAGE原理&quot;">​</a></h2><p>GraphSAGE 提出的前提是因为基于直推式 (transductive) 学习的图卷积网络无法适应工业界的大多数业务场景。我们知道的是，基于直推式学习的图卷积网络是通过拉普拉斯矩阵直接为图上的每个节点学习 embedding 表示，每次学习是针对于当前图上所有的节点。然而在实际的工业场景中，图中的结构和节点都不可能是固定的，会随着时间的变化而发生改变。例如在 Pinterest 公司的场景下，每分钟都会上传新的照片素材，同时也会有新用户不断的注册，那么图上的节点会不断的变化。在这样的场景中，直推式学习的方法就需要不断的重新训练才能够为新加入的节点学习 embedding，导致在实际场景中无法投入使用。</p><p>在这样的背景下，斯坦福大学提出了一种归纳 (inductive) 学习的 GCN 方法 ——GraphSAGE，即<strong>通过聚合邻居信息的方式为给定的节点学习 embedding</strong>。不同于直推式 (transductive) 学习，GraphSAGE 是通过学习聚合节点邻居生成节点 Embedding 的函数的方式，为任意节点学习 embedding，进而将 GCN 扩展成归纳学习任务。</p><p>对于想直接应用 GCN 或者 GraphSAGE 的我们而言，不用非要去理解其背后晦涩难懂的数学原理，可以仅从公式的角度来理解 GraphSAGE 的具体操作。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/swallown1/blogimages@main/images/image-20220423094435223.png" style="zoom:90%;"></div><p>上面这个公式可以非常直观的让我们理解 GraphSAGE 的原理。</p>',8),r={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},c={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.576ex"},xmlns:"http://www.w3.org/2000/svg",width:"2.291ex",height:"2.463ex",role:"img",focusable:"false",viewBox:"0 -833.9 1012.6 1088.7","aria-hidden":"true"},i=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(609,363) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(609,-247) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z" style="stroke-width:3;"></path></g></g></g></g>',1),E=[i],y=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msubsup",null,[s("mi",null,"h"),s("mi",null,"v"),s("mn",null,"0")])])],-1),d={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},m={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.576ex"},xmlns:"http://www.w3.org/2000/svg",width:"2.324ex",height:"2.508ex",role:"img",focusable:"false",viewBox:"0 -853.7 1027.4 1108.5","aria-hidden":"true"},Q=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(609,363) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(609,-247) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z" style="stroke-width:3;"></path></g></g></g></g>',1),T=[Q],h=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msubsup",null,[s("mi",null,"h"),s("mi",null,"v"),s("mi",null,"k")])])],-1),_={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},u={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"4.867ex",height:"2.262ex",role:"img",focusable:"false",viewBox:"0 -750 2151 1000","aria-hidden":"true"},g=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(888,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1277,0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1762,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g>',1),b=[g],F=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("mi",null,"N"),s("mo",{stretchy:"false"},"("),s("mi",null,"v"),s("mo",{stretchy:"false"},")")])],-1),w={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},x={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.576ex"},xmlns:"http://www.w3.org/2000/svg",width:"4.369ex",height:"2.508ex",role:"img",focusable:"false",viewBox:"0 -853.7 1931.1 1108.5","aria-hidden":"true"},f=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(609,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(521,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(1299,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mi" transform="translate(609,-247) scale(0.707)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g></g>',1),C=[f],A=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msubsup",null,[s("mi",null,"h"),s("mi",null,"u"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mi",null,"k"),s("mo",null,"−"),s("mn",null,"1")])])])],-1),D={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},k={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-1.238ex"},xmlns:"http://www.w3.org/2000/svg",width:"13.703ex",height:"3.78ex",role:"img",focusable:"false",viewBox:"0 -1123.8 6056.9 1670.9","aria-hidden":"true"},v=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="munder"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(1089,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(572,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1239,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2127,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2516,0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3001,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mfrac" transform="translate(3702.8,0)"><g data-mml-node="msubsup" transform="translate(494.3,520.2) scale(0.707)"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(609,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(521,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(1299,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mi" transform="translate(609,-247) scale(0.707)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mrow" transform="translate(220,-370.3) scale(0.707)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(278,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1166,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1555,0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2040,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2429,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width:3;"></path></g></g><rect width="2114.1" height="60" x="120" y="220"></rect></g></g></g>',1),B=[v],H=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("munder",null,[s("mo",{"data-mjx-texclass":"OP"},"∑"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mi",null,"u"),s("mo",null,"∈"),s("mi",null,"N"),s("mo",{stretchy:"false"},"("),s("mi",null,"v"),s("mo",{stretchy:"false"},")")])]),s("mfrac",null,[s("msubsup",null,[s("mi",null,"h"),s("mi",null,"u"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mi",null,"k"),s("mo",null,"−"),s("mn",null,"1")])]),s("mrow",null,[s("mo",{"data-mjx-texclass":"ORD",stretchy:"false"},"|"),s("mi",null,"N"),s("mo",{stretchy:"false"},"("),s("mi",null,"v"),s("mo",{stretchy:"false"},")"),s("mo",{"data-mjx-texclass":"ORD",stretchy:"false"},"|")])])])],-1),L=s("strong",null,"借助图上的边将邻居节点的信息通过边关系聚合到节点表示中 (简称卷积操作)",-1),M={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},S={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.576ex"},xmlns:"http://www.w3.org/2000/svg",width:"4.369ex",height:"2.508ex",role:"img",focusable:"false",viewBox:"0 -853.7 1931.1 1108.5","aria-hidden":"true"},G=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(609,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(521,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(1299,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mi" transform="translate(609,-247) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z" style="stroke-width:3;"></path></g></g></g></g>',1),P=[G],j=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msubsup",null,[s("mi",null,"h"),s("mi",null,"v"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mi",null,"k"),s("mo",null,"−"),s("mn",null,"1")])])])],-1),q=s("strong",null,"在对自身做多次非线性变换时，同时利用边关系聚合邻居节点信息。",-1),V={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},Z={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.357ex"},xmlns:"http://www.w3.org/2000/svg",width:"2.509ex",height:"1.902ex",role:"img",focusable:"false",viewBox:"0 -683 1108.9 840.8","aria-hidden":"true"},N=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(716,-150) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z" style="stroke-width:3;"></path></g></g></g></g>',1),z=[N],I=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msub",null,[s("mi",null,"Z"),s("mi",null,"v")])])],-1),R=s("p",null,"可以发现相比传统的方法 (MLP，CNN，DeepWalk 或 EGES)，GCN 或 GraphSAGE 存在一些优势：",-1),J=s("ol",null,[s("li",null,[n("相比于传统的深度学习方法 (MLP,CNN)，GCN 在对自身节点进行非线性变换时，同时考虑了图中的邻接关系。从 CNN 的角度理解，GCN 通过堆叠多层结构在图结构数据上拥有更大的"),s("strong",null,"感受野"),n("，利用更加广域内的信息。")]),s("li",null,"相比于图嵌入学习方法 (DeepWalk，EGES)，GCN 在学习节点表示的过程中，在利用节点自身的属性信息之外，更好的利用图结构上的边信息。相比于借助随机采样的方式来使用边信息，GCN 的方式能从全局的角度利用的邻居信息。此外，类似于 GraphSAGE 这种归纳 (inductive) 学习的 GCN 方法，通过学习聚合节点邻居生成节点 Embedding 的函数的方式，更适用于图结构和节点会不断变化的工业场景。")],-1),O=s("p",null,"在采样得到目标节点的邻居集之后，那么如何聚合邻居节点的信息来更新目标节点的嵌入表示呢？下面就来看看 GraphSAGE 中提及的四个聚合函数。",-1),W=s("h2",{id:"graphsage的采样和聚合",tabindex:"-1"},[n("GraphSAGE 的采样和聚合 "),s("a",{class:"header-anchor",href:"#graphsage的采样和聚合","aria-label":'Permalink to "GraphSAGE的采样和聚合"'},"​")],-1),U={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},X={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"4.867ex",height:"2.262ex",role:"img",focusable:"false",viewBox:"0 -750 2151 1000","aria-hidden":"true"},$=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(888,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1277,0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1762,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g>',1),K=[$],Y=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("mi",null,"N"),s("mo",{stretchy:"false"},"("),s("mi",null,"v"),s("mo",{stretchy:"false"},")")])],-1),ss=s("div",{align:"center"},[s("img",{src:"https://cdn.jsdelivr.net/gh/swallown1/blogimages@main/images/image-20220406135753358.png",style:{zoom:"90%"}})],-1),ns=s("p",null,"GraphSAGE 的 minibatch 算法的思路是针对 Batch 内的所有节点，通过采样和聚合节点，为每一个节点学习一个 embedding。",-1),as=s("h4",{id:"邻居采样",tabindex:"-1"},[n("邻居采样 "),s("a",{class:"header-anchor",href:"#邻居采样","aria-label":'Permalink to "邻居采样"'},"​")],-1),ls={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},ps={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"0"},xmlns:"http://www.w3.org/2000/svg",width:"2.738ex",height:"1.932ex",role:"img",focusable:"false",viewBox:"0 -853.7 1210.4 853.7","aria-hidden":"true"},es=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(792,363) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width:3;"></path></g></g></g></g>',1),os=[es],ts=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msup",null,[s("mi",null,"B"),s("mi",null,"k")])])],-1),rs={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},cs={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"0"},xmlns:"http://www.w3.org/2000/svg",width:"4.783ex",height:"1.932ex",role:"img",focusable:"false",viewBox:"0 -853.7 2114.1 853.7","aria-hidden":"true"},is=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(792,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(521,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(1299,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g></g></g></g>',1),Es=[is],ys=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msup",null,[s("mi",null,"B"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mi",null,"k"),s("mo",null,"−"),s("mn",null,"1")])])])],-1),ds={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},ms={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"0"},xmlns:"http://www.w3.org/2000/svg",width:"2.705ex",height:"1.887ex",role:"img",focusable:"false",viewBox:"0 -833.9 1195.6 833.9","aria-hidden":"true"},Qs=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(792,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" style="stroke-width:3;"></path></g></g></g></g></g>',1),Ts=[Qs],hs=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msup",null,[s("mi",null,"B"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mn",null,"0")])])])],-1),_s=s("strong",null,"为什么需要采样并且固定采样数量 S？",-1),us={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},gs={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"0"},xmlns:"http://www.w3.org/2000/svg",width:"2.705ex",height:"1.887ex",role:"img",focusable:"false",viewBox:"0 -833.9 1195.6 833.9","aria-hidden":"true"},bs=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(792,363) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" style="stroke-width:3;"></path></g></g></g></g>',1),Fs=[bs],ws=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msup",null,[s("mi",null,"B"),s("mn",null,"0")])])],-1),xs=s("p",null,"进行邻居采样并固定采样数量 S 主要是因为：1. 采样邻居节点避免了在全图的搜索以及使用全部邻居节点所导致计算复杂度高的问题；2. 可以通过采样使得部分节点更同质化，即两个相似的节点具有相同表达形式。3. 采样固定数量是保持每个 batch 的计算占用空间是固定的，方便进行批量训练。",-1),fs={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},Cs={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"0"},xmlns:"http://www.w3.org/2000/svg",width:"2.705ex",height:"1.887ex",role:"img",focusable:"false",viewBox:"0 -833.9 1195.6 833.9","aria-hidden":"true"},As=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(792,363) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" style="stroke-width:3;"></path></g></g></g></g>',1),Ds=[As],ks=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msup",null,[s("mi",null,"B"),s("mn",null,"0")])])],-1),vs=s("h4",{id:"聚合函数",tabindex:"-1"},[n("聚合函数 "),s("a",{class:"header-anchor",href:"#聚合函数","aria-label":'Permalink to "聚合函数"'},"​")],-1),Bs=s("p",null,[n("如何对于采样到的节点集进行聚合，介绍的 4 种方式：Mean 聚合、Convolutional 聚合、LSTM 聚合以及 Pooling 聚合。由于邻居节点是无序的，所以希望构造的聚合函数具有"),s("strong",null,"对称性 (即输出的结果不因输入排序的不同而改变)"),n("，同时拥有"),s("strong",null,"较强的表达能力"),n("。")],-1),Hs=s("strong",null," element-wise",-1),Ls={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},Ms={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.576ex"},xmlns:"http://www.w3.org/2000/svg",width:"4.369ex",height:"2.508ex",role:"img",focusable:"false",viewBox:"0 -853.7 1931.1 1108.5","aria-hidden":"true"},Ss=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(609,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(521,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(1299,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mi" transform="translate(609,-247) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z" style="stroke-width:3;"></path></g></g></g></g>',1),Gs=[Ss],Ps=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msubsup",null,[s("mi",null,"h"),s("mi",null,"v"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mi",null,"k"),s("mo",null,"−"),s("mn",null,"1")])])])],-1),js={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},qs={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.576ex"},xmlns:"http://www.w3.org/2000/svg",width:"21.534ex",height:"2.508ex",role:"img",focusable:"false",viewBox:"0 -853.7 9518 1108.5","aria-hidden":"true"},Vs=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1051,0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1815,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2565,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3453,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="msubsup" transform="translate(3842,0)"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(609,363) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(609,-247) scale(0.707)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(4905.5,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(5183.5,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(6033.2,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(6978,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(7866,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(8255,0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(8740,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(9129,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g>',1),Zs=[Vs],Ns=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("mi",null,"M"),s("mi",null,"E"),s("mi",null,"A"),s("mi",null,"N"),s("mo",{stretchy:"false"},"("),s("msubsup",null,[s("mi",null,"h"),s("mi",null,"u"),s("mi",null,"k")]),s("mo",{"data-mjx-texclass":"ORD",stretchy:"false"},"|"),s("mi",null,"u"),s("mo",null,"∈"),s("mi",null,"N"),s("mo",{stretchy:"false"},"("),s("mi",null,"v"),s("mo",{stretchy:"false"},")"),s("mo",{stretchy:"false"},")")])],-1),zs=s("strong",null," 分别",-1),Is=s("strong",null,"相加",-1),Rs=a("<li><p>Convolutional 聚合：这是一种基于 GCN 聚合方式的变种，首先对邻居节点特征和自身节点特征求均值，得到的聚合特征送入到全连接网络中。与 Mean 不同的是，这里<strong>只经过一个全连接层</strong>。</p></li><li><p>LSTM 聚合：由于 LSTM 可以捕捉到序列信息，因此相比于 Mean 聚合，这种聚合方式的<strong>表达能力更强</strong>；但由于 LSTM 对于输入是有序的，因此该方法不具备<strong>对称性</strong>。作者对于无序的节点进行随机排列以调整 LSTM 所需的有序性。</p></li><li><p>Pooling 聚合：对于邻居节点和中心节点进行一次非线性转化，将结果进行一次基于<strong> element-wise</strong> 的<strong>最大池化</strong>操作。该种方式具有<strong>较强的表达能力</strong>的同时还具有<strong>对称性</strong>。</p></li>",3),Js=a('<p>综上，可以发现 GraphSAGE 之所以可以用于大规模的工业场景，主要是因为模型主要是通过学习聚合函数，通过归纳式的学习方法为节点学习特征表示。接下来看看 PinSAGE 的主要内容。</p><h2 id="pinsage" tabindex="-1">PinSAGE <a class="header-anchor" href="#pinsage" aria-label="Permalink to &quot;PinSAGE&quot;">​</a></h2><h3 id="背景" tabindex="-1">背景 <a class="header-anchor" href="#背景" aria-label="Permalink to &quot;背景&quot;">​</a></h3><p>PinSAGE 模型是 Pinterest 在 GraphSAGE 的基础上实现的可以应用于实际工业场景的召回算法。Pinterest 公司的主要业务是采用瀑布流的形式向用户展现图片，无需用户翻页，新的图片会自动加载。因此在 Pinterest 网站上，有大量的图片 (被称为 pins)，而用户可以将喜欢的图片分类，即将 pins 钉在画板 boards 上。可以发现基于这样的场景，pin 相当于普通推荐场景中 item，用户<strong>钉</strong>的行为可以认为是用于的交互行为。于是 PinSAGE 模型主要应用的思路是，基于 GraphSAGE 的原理学习到聚合方法，并为每个图片 (pin) 学习一个向量表示，然后基于 pin 的向量表示做<strong> item2item 的召回</strong>。</p><p>可以知道的是，PinSAGE 是在 GraphSAGE 的基础上进行改进以适应实际的工业场景，因此除了改进卷积操作中的邻居采样策略以及聚合函数的同时还有一些工程技巧上的改进，使得在大数据场景下能更快更好的进行模型训练。因此在了解 GraphSAGE 的原理后，我们详细的了解一下本文的主要改进以及与 GraphSAGE 的区别。</p><h3 id="重要性采样" tabindex="-1">重要性采样 <a class="header-anchor" href="#重要性采样" aria-label="Permalink to &quot;重要性采样&quot;">​</a></h3><p>在实际场景当中，一个 item 可能被数以百万，千万的用户交互过，所以不可能聚合所有邻居节点是不可行的，只可能是采样部分邻居进行信息聚合。但是如果采用 GraphSAGE 中随机采样的方法，由于采样的邻居有限 (这里是相对于所有节点而言)，会存在一定的偏差。因此 PinSAGE 在采样中考虑了更加重要的邻居节点，即卷积时只注重部分重要的邻居节点信息，已达到高效计算的同时又可以消除偏置。</p><p>PinSAGE 使用重要性采样方法，即需要为每个邻居节点计算一个重要性权重，根据权重选取 top-t 的邻居作为聚合时的邻居集合。其中计算重要性的过程是，以目标节点为起点，进行 random-walk，采样结束之后计算所有节点访问数的 L1-normalized 作为重要性权重，同时这个权重也会在聚合过程中加以使用 (<strong>加权聚合</strong>)。</p><p>这里对于 ** 计算权重之后如何得到 top-t 的邻居节点，** 原文并没有直接的叙述。这里可以有两种做法，第一种就是直接采用重要权重，这种方法言简意赅，比较直观。第二种做法就是对游走得到的所有邻居进行随机抽样，而计算出的权重可以用于聚合阶段。个人理解第二种做法的可行性出于两点原因，其一是这样方法可以避免存在一些 item 由于权重系数低永远不会被选中的问题；其二可能并不是将所有重要性的邻居进行聚合更合理，毕竟重要性权重是通过随机采样而得到的，具有一定的随机性。当然以上两种方法都是可行的方案，可以通过尝试看看具体哪种方法会更有效。</p><h3 id="聚合函数-1" tabindex="-1">聚合函数 <a class="header-anchor" href="#聚合函数-1" aria-label="Permalink to &quot;聚合函数&quot;">​</a></h3><p>PinSAGE 中提到的 Convolve 算法（单层图卷积操作）相当于 GraphSAGE 算法的聚合过程，在实际执行过程中通过对每一层执行一次图卷积操作以得到不同阶邻居的信息，具体过程如下图所示：</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/swallown1/blogimages@main/images/image-20220406202027832.png" style="zoom:110%;"></div><p>上述的单层图卷积过程如下三步：</p>',13),Os={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},Ws={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.489ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.229ex",height:"1.486ex",role:"img",focusable:"false",viewBox:"0 -441 543 657","aria-hidden":"true"},Us=s("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[s("g",{"data-mml-node":"math"},[s("g",{"data-mml-node":"mi"},[s("path",{"data-c":"1D6FE",d:"M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z",style:{"stroke-width":"3"}})])])],-1),Xs=[Us],$s=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("mi",null,"γ")])],-1),Ks=s("strong",null,"加权和",-1),Ys={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},sn={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.357ex"},xmlns:"http://www.w3.org/2000/svg",width:"2.155ex",height:"1.357ex",role:"img",focusable:"false",viewBox:"0 -442 952.5 599.8","aria-hidden":"true"},nn=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g></g>',1),an=[nn],ln=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msub",null,[s("mi",null,"z"),s("mi",null,"u")])])],-1),pn={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},en={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.357ex"},xmlns:"http://www.w3.org/2000/svg",width:"2.46ex",height:"1.357ex",role:"img",focusable:"false",viewBox:"0 -442 1087.5 599.8","aria-hidden":"true"},on=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(633,-150) scale(0.707)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g></g>',1),tn=[on],rn=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msub",null,[s("mi",null,"n"),s("mi",null,"u")])])],-1),cn={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},En={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.357ex"},xmlns:"http://www.w3.org/2000/svg",width:"2.155ex",height:"1.357ex",role:"img",focusable:"false",viewBox:"0 -442 952.5 599.8","aria-hidden":"true"},yn=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g></g>',1),dn=[yn],mn=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msub",null,[s("mi",null,"z"),s("mi",null,"u")])])],-1),Qn={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},Tn={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.357ex"},xmlns:"http://www.w3.org/2000/svg",width:"2.155ex",height:"1.357ex",role:"img",focusable:"false",viewBox:"0 -442 952.5 599.8","aria-hidden":"true"},hn=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g></g>',1),_n=[hn],un=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msub",null,[s("mi",null,"z"),s("mi",null,"u")])])],-1),gn=s("strong",null,"可以使训练更稳定，以及在近似查找最近邻的应用中更有效率。",-1),bn=a('<h3 id="基于mini-batch堆叠多层图卷积" tabindex="-1">基于<strong> mini-batch</strong> 堆叠多层图卷积 <a class="header-anchor" href="#基于mini-batch堆叠多层图卷积" aria-label="Permalink to &quot;基于**mini-batch**堆叠多层图卷积&quot;">​</a></h3><p>与 GraphSAGE 类似，采用的是基于 mini-batch 的方式进行训练。之所以这么做的原因是因为什么呢？在实际的工业场景中，由于用户交互图非常庞大，无法对于所有的节点同时学习一个 embedding，因此需要从原始图上寻找与 mini-batch 节点相关的子图。具体地是说，对于 mini-batch 内的所有节点，会通过采样的方式逐层的寻找相关邻居节点，再通过对每一层的节点做一次图卷积操作，以从 k 阶邻居节点聚合信息。</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/swallown1/blogimages@main/images/image-20220406204431024.png" style="zoom:60%;"></div><p>如上图所示：对于 batch 内的所有节点 (图上最顶层的 6 个节点)，依次根据权重采样，得到 batch 内所有节点的一阶邻居 (图上第二层的所有节点)；然后对于所有一阶邻居再次进行采样，得到所有二阶邻居 (图上的最后一层)。节点采样阶段完成之后，与采样的顺序相反进行聚合操作。首先对二阶邻居进行单次图卷积，将二阶节点信息聚合已更新一阶节点的向量表示 (其中小方块表示的是一层非线性转化)；其次对一阶节点再次进行图卷积操作，将一阶节点的信息聚合已更新 batch 内所有节点的向量表示。仅此对于一个 batch 内的所有的样本通过卷积操作学习到一个 embedding，而每一个 batch 的学习过程中仅<strong>利用与 mini-batch 内相关节点的子图结构。</strong></p><h3 id="训练过程" tabindex="-1"><strong>训练过程</strong> <a class="header-anchor" href="#训练过程" aria-label="Permalink to &quot;**训练过程**&quot;">​</a></h3><p>PinSage 在训练时采用的是 Margin Hinge Loss 损失函数，主要的思想是最大化正例 embedding 之间的相关性，同时还要保证负例之间相关性相比正例之间的相关性小于某个阈值 (Margin)。具体的公式如下：</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/swallown1/blogimages@main/images/image-20220406210833675.png" style="zoom:100%;"></div>',7),Fn={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},wn={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.65ex"},xmlns:"http://www.w3.org/2000/svg",width:"2.538ex",height:"2.195ex",role:"img",focusable:"false",viewBox:"0 -683 1121.7 970.2","aria-hidden":"true"},xn=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(716,-150) scale(0.707)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z" style="stroke-width:3;"></path></g></g></g></g>',1),fn=[xn],Cn=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msub",null,[s("mi",null,"Z"),s("mi",null,"p")])])],-1),An={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},Dn={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.357ex"},xmlns:"http://www.w3.org/2000/svg",width:"2.285ex",height:"1.902ex",role:"img",focusable:"false",viewBox:"0 -683 1010 840.8","aria-hidden":"true"},kn=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(716,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g></g>',1),vn=[kn],Bn=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msub",null,[s("mi",null,"Z"),s("mi",null,"i")])])],-1),Hn={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},Ln={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.592ex"},xmlns:"http://www.w3.org/2000/svg",width:"3.415ex",height:"2.137ex",role:"img",focusable:"false",viewBox:"0 -683 1509.5 944.6","aria-hidden":"true"},Mn=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(716,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(633,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width:3;"></path></g></g></g></g></g></g>',1),Sn=[Mn],Gn=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msub",null,[s("mi",null,"Z"),s("mrow",{"data-mjx-texclass":"ORD"},[s("msub",null,[s("mi",null,"n"),s("mi",null,"k")])])])])],-1),Pn={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},jn={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"0"},xmlns:"http://www.w3.org/2000/svg",width:"1.885ex",height:"1.62ex",role:"img",focusable:"false",viewBox:"0 -716 833 716","aria-hidden":"true"},qn=s("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[s("g",{"data-mml-node":"math"},[s("g",{"data-mml-node":"mi"},[s("path",{"data-c":"394",d:"M51 0Q46 4 46 7Q46 9 215 357T388 709Q391 716 416 716Q439 716 444 709Q447 705 616 357T786 7Q786 4 781 0H51ZM507 344L384 596L137 92L383 91H630Q630 93 507 344Z",style:{"stroke-width":"3"}})])])],-1),Vn=[qn],Zn=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("mi",{mathvariant:"normal"},"Δ")])],-1),Nn=a(`<p>对于正样本而言，文中的定义是如果用户在点击的 item q 之后立即点击了 item i，即认为 &lt;q, i&gt; 构成正样本对。直观的我们很好理解这句话，不过在参考 DGL 中相关代码实现时，发现这部分的内容和原文中有一定的出入。具体地，代码中将所有的训练样本构造成用户 - 项目二部图，然后对 batch 内的每个 item q，根据 item-user-item 的元路径进行随机游走，得到被同一个用户交互过的 item i，因此组成 &lt; q,i &gt; 正样本对。对于负样本部分，相对来说更为重要，因此内容相对比较多，将在下面的负样本生成部分详细介绍。</p><p>这里还有一个比较重要的细节需要注意，由于模型是用于 item to item 的召回，因此优化目标是与正样本之间的表示尽可能的相近，与负样本之间的表示尽可能的远。而图卷积操作会使得具有邻接关系的节点表示具有同质性，因此结合这两点，就需要在构建图结构的时，要将<strong>训练样本之间可能存在的边在二部图上删除</strong>，避免因为边的存在使得因卷积操作而导致的信息泄露。</p><h3 id="工程技巧" tabindex="-1">工程技巧 <a class="header-anchor" href="#工程技巧" aria-label="Permalink to &quot;工程技巧&quot;">​</a></h3><p>由于 PinSAGE 是一篇工业界的论文，其中会涉及与实际工程相关的内容，这里在了解完算法思想之后，再从实际落地的角度看看 PinSAGE 给我们介绍的工程技巧。</p><p><strong>负样本的生成</strong></p><p>召回模型最主要的任务是从候选集合中选出用户可能感兴趣的 item，直观的理解就是让模型将用户喜欢的和不喜欢的进行区分。然而由于候选集合的庞大数量，许多 item 之间十分相似，导致模型划分出来用户喜欢的 item 中会存在一些难以区分的 item (即与用户非常喜欢 item 比较相似的那一部分)。因此对于召回模型不仅能区分用户喜欢和不喜欢的 item，同时还能区分与用户喜欢的 item 十分相似的那一部分 item。那么如果做到呢？这主要是交给 easy negative examples 和 hard negative examples 两种负样本给模型学习。</p><ul><li>easy 负样本：这里对于 mini-batch 内的所有 pair (训练样本对) 会共享 500 负样本，这 500 个样本从 batch 之外的所有节点中随机采样得到。这么做可以减少在每个 mini-batch 中因计算所有节点的 embedding 所需的时间，文中指出这和为每个 item 采样一定数量负样本无差异。</li><li>hard 负样本：这里使用 hard 负样本的原因是根据实际场景的问题出发，模型需要从 20 亿的物品 item 集合中识别出最相似的 1000 个，即模型需要从 2 百万 item 中识别出最相似的那一个 item。也就是说模型的区分能力不够细致，为了解决这个问题，加入了一些 hard 样本。对于 hard 负样本，应该是与 q 相似 以及和 i 不相似的物品，具体地的生成方式是将图上的节点计算相对节点 q 的个性化 PageRank 分值，根据分值的排序随机从 2000~5000 的位置选取节点作为负样本。</li></ul><p>负样本的构建是召回模型的中关键的内容，在各家公司的工作都予以体现，具体的大家可以参考 Facebook 发表的<a href="https://arxiv.org/pdf/2006.11632v1.pdf" target="_blank" rel="noreferrer">《Embedding-based Retrieval in Facebook Search》</a></p><p><strong>渐进式训练 (Curriculum training)</strong></p><p>由于 hard 负样本的加入，模型的训练时间加长（由于与 q 过于相似，导致 loss 比较小，导致梯度更新的幅度比较小，训练起来比较慢），那么渐进式训练就是为了来解决这个问题。</p><p>如何渐进式：先在第一轮训练使用 easy 负样本，帮助模型先快速收敛 (先让模型有个最基本的分辨能力) 到一定范围，然后在逐步分加入 hard 负样本 (方式是在第 n 轮训练时给每个物品的负样本集合增加 n-1 个 hard 负样本)，以调整模型细粒度的区分能力 (让模型能够区分相似的 item)。</p><p><strong>节点特征 (side information)</strong></p><p>这里与 EGES 的不同，这里的边信息不是端到端训练得到，而是通过事前的预处理得到的。对于每个节点 (即 pin)，都会有一个图片和一点文本信息。因此对于每个节点使用图片的向量、文字的向量以及节点的度拼接得到。这里其实也解释了为什么在图卷积操作时，会先进行一个非线性转化，其实就是将不同空间的特征进行转化 (融合)。</p><p><strong>构建 mini-batch</strong></p><p>不同于常规的构建方式，PinSAGE 中构建 mini-batch 的方式是基于生产者消费者模式。什么意思的，就是将 CPU 和 GPU 分开工作，让 CPU 负责取特征，重建索引，邻接列表，负采样等工作，让 GPU 进行矩阵运算，即 CPU 负责生产每个 batch 所需的所有数据，GPU 则根据 CPU 生产的数据进行消费 (运算)。这样由于考虑 GPU 的利用率，无法将所有特征矩阵放在 GPU，只能存在 CPU 中，然而每次查找会导致非常耗时，通过上面的方式使得图卷积操作过程中就没有 GPU 与 CPU 的通信需求。</p><p><strong>多 GPU 训练超大 batch</strong></p><p>前向传播过程中，各个 GPU 等分 minibatch，共享一套模型参数；反向传播时，将每个 GPU 中的参数梯度都聚合到一起，同步执行 SGD。为了保证因海量数据而使用的超大 batchsize 的情况下模型快速收敛以及泛化精度，采用 warmup 过程，即在第一个 epoch 中将学习率线性提升到最高，后面的 epoch 中再逐步指数下降。</p><p><strong>使用 MapReduce 高效推断</strong></p><p>在模型训练结束之后，需要为所有节点计算一个 embedding，如果按照训练过程中的前向传播过程来生成，会存在大量重复的计算。因为当计算一个节点的 embedding 的时候，其部分邻居节点已经计算过了，同时如果该节点作为其他节点邻居时，也会被再次计算。针对这个问题，本文采用 MapReduce 的方法进行推断。该过程主要分为两步，具体如下图所示：</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/swallown1/blogimages@main/images/image-20220407132111547.png" style="zoom:60%;"></div><ol><li>将 item 的 embedding 进行聚合，即利用 item 的图片、文字和度等信息的表示进行 join (拼接)，在通过一层 dense 后得到 item 的低维向量。</li><li>然后根据 item 来匹配其一阶邻居 (join)，然后根据 item 进行 pooling (其实就是 GroupBy pooling)，得到一次图卷积操作。通过堆叠多次直接得到全量的 embedding。</li></ol><p>其实这块主要就是通过 MapReduce 的大数据处理能力，直接对全量节点进行一次运算得到其 embedding，避免了分 batch 所导致的重复计算。</p><h2 id="代码解析" tabindex="-1">代码解析 <a class="header-anchor" href="#代码解析" aria-label="Permalink to &quot;代码解析&quot;">​</a></h2><p>了解完基本的原理之后，最关键的还是得解析源码，以证实上面讲的细节的准确性。下面基于 DGL 中实现的代码，看看模型中的一些细节。</p><h3 id="数据处理" tabindex="-1">数据处理 <a class="header-anchor" href="#数据处理" aria-label="Permalink to &quot;数据处理&quot;">​</a></h3><p>在弄清楚模型之前，最重要的就是知道送入模型的数据到底是什么养的，以及 PinSAGE 相对于 GraphSAGE 最大的区别就在于如何采样邻居，如何构建负样本等。</p><p>首先需要明确的是，无论是<strong>邻居采样</strong>还是<strong>样本的构造</strong>都发生在图结构上，因此最主要的是需要先构建一个 user 和 item 组成的二部图。</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#6A737D;"># ratings是所有的用户交互</span></span>
<span class="line"><span style="color:#6A737D;"># 过滤掉为出现在交互中的用户和项目</span></span>
<span class="line"><span style="color:#E1E4E8;">distinct_users_in_ratings </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> ratings[</span><span style="color:#9ECBFF;">&#39;user_id&#39;</span><span style="color:#E1E4E8;">].unique()</span></span>
<span class="line"><span style="color:#E1E4E8;">distinct_movies_in_ratings </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> ratings[</span><span style="color:#9ECBFF;">&#39;movie_id&#39;</span><span style="color:#E1E4E8;">].unique()</span></span>
<span class="line"><span style="color:#E1E4E8;">users </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> users[users[</span><span style="color:#9ECBFF;">&#39;user_id&#39;</span><span style="color:#E1E4E8;">].isin(distinct_users_in_ratings)]</span></span>
<span class="line"><span style="color:#E1E4E8;">movies </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> movies[movies[</span><span style="color:#9ECBFF;">&#39;movie_id&#39;</span><span style="color:#E1E4E8;">].isin(distinct_movies_in_ratings)]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 将电影特征分组 genres (a vector), year (a category), title (a string)</span></span>
<span class="line"><span style="color:#E1E4E8;">genre_columns </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> movies.columns.drop([</span><span style="color:#9ECBFF;">&#39;movie_id&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;title&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;year&#39;</span><span style="color:#E1E4E8;">])</span></span>
<span class="line"><span style="color:#E1E4E8;">movies[genre_columns] </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> movies[genre_columns].fillna(</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">).astype(</span><span style="color:#9ECBFF;">&#39;bool&#39;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">movies_categorical </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> movies.drop(</span><span style="color:#9ECBFF;">&#39;title&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">axis</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">## 构建图</span></span>
<span class="line"><span style="color:#E1E4E8;">graph_builder </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> PandasGraphBuilder()</span></span>
<span class="line"><span style="color:#E1E4E8;">graph_builder.add_entities(users, </span><span style="color:#9ECBFF;">&#39;user_id&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;user&#39;</span><span style="color:#E1E4E8;">)  </span><span style="color:#6A737D;"># 添加user类型节点</span></span>
<span class="line"><span style="color:#E1E4E8;">graph_builder.add_entities(movies_categorical, </span><span style="color:#9ECBFF;">&#39;movie_id&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;movie&#39;</span><span style="color:#E1E4E8;">)  </span><span style="color:#6A737D;"># 添加movie类型节点</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">#  构建用户-电影的无向图</span></span>
<span class="line"><span style="color:#E1E4E8;">graph_builder.add_binary_relations(ratings, </span><span style="color:#9ECBFF;">&#39;user_id&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;movie_id&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;watched&#39;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">graph_builder.add_binary_relations(ratings, </span><span style="color:#9ECBFF;">&#39;movie_id&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;user_id&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;watched-by&#39;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">g </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> graph_builder.build()</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#6A737D;"># ratings是所有的用户交互</span></span>
<span class="line"><span style="color:#6A737D;"># 过滤掉为出现在交互中的用户和项目</span></span>
<span class="line"><span style="color:#24292E;">distinct_users_in_ratings </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> ratings[</span><span style="color:#032F62;">&#39;user_id&#39;</span><span style="color:#24292E;">].unique()</span></span>
<span class="line"><span style="color:#24292E;">distinct_movies_in_ratings </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> ratings[</span><span style="color:#032F62;">&#39;movie_id&#39;</span><span style="color:#24292E;">].unique()</span></span>
<span class="line"><span style="color:#24292E;">users </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> users[users[</span><span style="color:#032F62;">&#39;user_id&#39;</span><span style="color:#24292E;">].isin(distinct_users_in_ratings)]</span></span>
<span class="line"><span style="color:#24292E;">movies </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> movies[movies[</span><span style="color:#032F62;">&#39;movie_id&#39;</span><span style="color:#24292E;">].isin(distinct_movies_in_ratings)]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 将电影特征分组 genres (a vector), year (a category), title (a string)</span></span>
<span class="line"><span style="color:#24292E;">genre_columns </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> movies.columns.drop([</span><span style="color:#032F62;">&#39;movie_id&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;title&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;year&#39;</span><span style="color:#24292E;">])</span></span>
<span class="line"><span style="color:#24292E;">movies[genre_columns] </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> movies[genre_columns].fillna(</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">).astype(</span><span style="color:#032F62;">&#39;bool&#39;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">movies_categorical </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> movies.drop(</span><span style="color:#032F62;">&#39;title&#39;</span><span style="color:#24292E;">, </span><span style="color:#E36209;">axis</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">## 构建图</span></span>
<span class="line"><span style="color:#24292E;">graph_builder </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> PandasGraphBuilder()</span></span>
<span class="line"><span style="color:#24292E;">graph_builder.add_entities(users, </span><span style="color:#032F62;">&#39;user_id&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;user&#39;</span><span style="color:#24292E;">)  </span><span style="color:#6A737D;"># 添加user类型节点</span></span>
<span class="line"><span style="color:#24292E;">graph_builder.add_entities(movies_categorical, </span><span style="color:#032F62;">&#39;movie_id&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;movie&#39;</span><span style="color:#24292E;">)  </span><span style="color:#6A737D;"># 添加movie类型节点</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">#  构建用户-电影的无向图</span></span>
<span class="line"><span style="color:#24292E;">graph_builder.add_binary_relations(ratings, </span><span style="color:#032F62;">&#39;user_id&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;movie_id&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;watched&#39;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">graph_builder.add_binary_relations(ratings, </span><span style="color:#032F62;">&#39;movie_id&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;user_id&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;watched-by&#39;</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">g </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> graph_builder.build()</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><p>在构建完原图之后，需要将交互数据 (ratings) 分成训练集和测试集，然后根据测试集从原图中抽取出与训练集中相关节点的子图。</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#6A737D;"># train_test_split_by_time 根据时间划分训练集和测试集</span></span>
<span class="line"><span style="color:#6A737D;"># 将用户的倒数第二次交互作为验证，最后一次交互用作测试</span></span>
<span class="line"><span style="color:#6A737D;"># train_indices 为用于训练的用户与电影的交互</span></span>
<span class="line"><span style="color:#E1E4E8;">train_indices, val_indices, test_indices </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> train_test_split_by_time(ratings, </span><span style="color:#9ECBFF;">&#39;timestamp&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;user_id&#39;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 只使用训练交互来构建图形，测试集相关的节点不应该出现在训练过程中。</span></span>
<span class="line"><span style="color:#6A737D;"># 从原图中提取与训练集相关节点的子图</span></span>
<span class="line"><span style="color:#E1E4E8;">train_g </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> build_train_graph(g, train_indices, </span><span style="color:#9ECBFF;">&#39;user&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;movie&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;watched&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;watched-by&#39;</span><span style="color:#E1E4E8;">)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#6A737D;"># train_test_split_by_time 根据时间划分训练集和测试集</span></span>
<span class="line"><span style="color:#6A737D;"># 将用户的倒数第二次交互作为验证，最后一次交互用作测试</span></span>
<span class="line"><span style="color:#6A737D;"># train_indices 为用于训练的用户与电影的交互</span></span>
<span class="line"><span style="color:#24292E;">train_indices, val_indices, test_indices </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> train_test_split_by_time(ratings, </span><span style="color:#032F62;">&#39;timestamp&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;user_id&#39;</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 只使用训练交互来构建图形，测试集相关的节点不应该出现在训练过程中。</span></span>
<span class="line"><span style="color:#6A737D;"># 从原图中提取与训练集相关节点的子图</span></span>
<span class="line"><span style="color:#24292E;">train_g </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> build_train_graph(g, train_indices, </span><span style="color:#032F62;">&#39;user&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;movie&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;watched&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;watched-by&#39;</span><span style="color:#24292E;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><h3 id="正负样本采样" tabindex="-1">正负样本采样 <a class="header-anchor" href="#正负样本采样" aria-label="Permalink to &quot;正负样本采样&quot;">​</a></h3><p>在得到训练图结构之后，为了进行 PinSAGE 提出的 item2item 召回任务，需要构建相应的训练样本。对于训练样本主要是构建正样本对和负样本对，前面我们已经提到了正样本对是基于 item to user to item 的随即游走得到的；对于负样本 DGL 的实现主要是随机采样，即只有 easy sample，未实现 hard sample。具体地，DGL 中主要是通过 sampler_module.ItemToItemBatchSampler 方法进行采样，主要代码如下：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#F97583;">class</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">ItemToItemBatchSampler</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">IterableDataset</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(self, g, user_type, item_type, batch_size):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.g </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> g</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.user_type </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> user_type</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.item_type </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> item_type</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.user_to_item_etype </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">list</span><span style="color:#E1E4E8;">(g.metagraph()[user_type][item_type])[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.item_to_user_etype </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">list</span><span style="color:#E1E4E8;">(g.metagraph()[item_type][user_type])[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.batch_size </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> batch_size</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">__iter__</span><span style="color:#E1E4E8;">(self):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">while</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># 随机采样batch_size个节点作为head  即论文中的q</span></span>
<span class="line"><span style="color:#E1E4E8;">            heads </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.randint(</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.g.number_of_nodes(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.item_type), (</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.batch_size,))</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># 本次元路径表示从item游走到user，再从user游走到item，总共二跳，取出二跳节点(电影节点)作为tails(即正样本)</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># 得到与heads被同一个用户消费过的其他item，做正样本</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># 这么做可能存在问题，</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;">#   1. 这种游走肯定会使正样本集中于少数热门item；</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;">#   2. 如果item只被一个用户消费过，二跳游走岂不是又回到起始item，这种case还是要处理的</span></span>
<span class="line"><span style="color:#E1E4E8;">            tails </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> dgl.sampling.random_walk(</span></span>
<span class="line"><span style="color:#E1E4E8;">                </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.g,</span></span>
<span class="line"><span style="color:#E1E4E8;">                heads,</span></span>
<span class="line"><span style="color:#E1E4E8;">                </span><span style="color:#FFAB70;">metapath</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">[</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.item_to_user_etype, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.user_to_item_etype])[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">][:, </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># 随机采样做负样本， 没有hard negative</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># 这么做会存在被同一个用户交互过的movie也会作为负样本</span></span>
<span class="line"><span style="color:#E1E4E8;">            neg_tails </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.randint(</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.g.number_of_nodes(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.item_type), (</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.batch_size,))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">            mask </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> (tails </span><span style="color:#F97583;">!=</span><span style="color:#E1E4E8;"> </span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#F97583;">yield</span><span style="color:#E1E4E8;"> heads[mask], tails[mask], neg_tails[mask]</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">ItemToItemBatchSampler</span><span style="color:#24292E;">(</span><span style="color:#6F42C1;">IterableDataset</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">(self, g, user_type, item_type, batch_size):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.g </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> g</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.user_type </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> user_type</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.item_type </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> item_type</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.user_to_item_etype </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">list</span><span style="color:#24292E;">(g.metagraph()[user_type][item_type])[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.item_to_user_etype </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">list</span><span style="color:#24292E;">(g.metagraph()[item_type][user_type])[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.batch_size </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> batch_size</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__iter__</span><span style="color:#24292E;">(self):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">while</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">True</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 随机采样batch_size个节点作为head  即论文中的q</span></span>
<span class="line"><span style="color:#24292E;">            heads </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.randint(</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.g.number_of_nodes(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.item_type), (</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.batch_size,))</span></span>
<span class="line"><span style="color:#24292E;">            </span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 本次元路径表示从item游走到user，再从user游走到item，总共二跳，取出二跳节点(电影节点)作为tails(即正样本)</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 得到与heads被同一个用户消费过的其他item，做正样本</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 这么做可能存在问题，</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;">#   1. 这种游走肯定会使正样本集中于少数热门item；</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;">#   2. 如果item只被一个用户消费过，二跳游走岂不是又回到起始item，这种case还是要处理的</span></span>
<span class="line"><span style="color:#24292E;">            tails </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> dgl.sampling.random_walk(</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.g,</span></span>
<span class="line"><span style="color:#24292E;">                heads,</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#E36209;">metapath</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">[</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.item_to_user_etype, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.user_to_item_etype])[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">][:, </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#24292E;">            </span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 随机采样做负样本， 没有hard negative</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 这么做会存在被同一个用户交互过的movie也会作为负样本</span></span>
<span class="line"><span style="color:#24292E;">            neg_tails </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.randint(</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.g.number_of_nodes(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.item_type), (</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.batch_size,))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">            mask </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> (tails </span><span style="color:#D73A49;">!=</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">yield</span><span style="color:#24292E;"> heads[mask], tails[mask], neg_tails[mask]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br></div></div><p>上面的样本采样过程只是一个简单的示例，如果面对实际问题，需要自己来重新完成这部分的内容。</p><h3 id="邻居节点采样" tabindex="-1">邻居节点采样 <a class="header-anchor" href="#邻居节点采样" aria-label="Permalink to &quot;邻居节点采样&quot;">​</a></h3><p>再得到训练样本之后，接下来主要是在训练图上，为 heads 节点采用其邻居节点。在 DGL 中主要是通过 sampler_module.NeighborSampler 来实现，具体地，通过<strong> sample_blocks</strong> 方法回溯生成各层卷积需要的 block，即所有的邻居集合。其中需要注意的几个地方，基于随机游走的重要邻居采样，DGL 已经实现，具体参考 **<a href="https://link.zhihu.com/?target=https%3A//docs.dgl.ai/generated/dgl.sampling.PinSAGESampler.html%3Fhighlight%3Dpinsagesampler" target="_blank" rel="noreferrer">dgl.sampling.PinSAGESampler</a>**，其次避免信息泄漏，代码中，先将 head → tails,head → neg_tails 从 frontier 中先删除，再生成 block。</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#F97583;">class</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">NeighborSampler</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">object</span><span style="color:#E1E4E8;">):  </span><span style="color:#6A737D;"># 图卷积的邻居采样</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(self, g, user_type, item_type, random_walk_length, random_walk_restart_prob,</span></span>
<span class="line"><span style="color:#E1E4E8;">                 num_random_walks, num_neighbors, num_layers):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.g </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> g</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.user_type </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> user_type</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.item_type </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> item_type</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.user_to_item_etype </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">list</span><span style="color:#E1E4E8;">(g.metagraph()[user_type][item_type])[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.item_to_user_etype </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">list</span><span style="color:#E1E4E8;">(g.metagraph()[item_type][user_type])[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 每层都有一个采样器，根据随机游走来决定某节点邻居的重要性(主要的实现已封装在PinSAGESampler中)</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 可以认为经过多次游走，落脚于某邻居节点的次数越多，则这个邻居越重要，就更应该优先作为邻居</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.samplers </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span></span>
<span class="line"><span style="color:#E1E4E8;">            dgl.sampling.PinSAGESampler(g, item_type, user_type, random_walk_length,</span></span>
<span class="line"><span style="color:#E1E4E8;">                random_walk_restart_prob, num_random_walks, num_neighbors)</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> _ </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">range</span><span style="color:#E1E4E8;">(num_layers)]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">sample_blocks</span><span style="color:#E1E4E8;">(self, seeds, heads</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">, tails</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">, neg_tails</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#9ECBFF;">&quot;&quot;&quot;根据随机游走得到的重要性权重，进行邻居采样&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">        blocks </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> []</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> sampler </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.samplers:</span></span>
<span class="line"><span style="color:#E1E4E8;">            frontier </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> sampler(seeds) </span><span style="color:#6A737D;"># 通过随机游走进行重要性采样，生成中间状态</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#F97583;">if</span><span style="color:#E1E4E8;"> heads </span><span style="color:#F97583;">is</span><span style="color:#E1E4E8;"> </span><span style="color:#F97583;">not</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#E1E4E8;">                </span><span style="color:#6A737D;"># 如果是在训练，需要将heads-&gt;tails 和 head-&gt;neg_tails这些待预测的边都去掉</span></span>
<span class="line"><span style="color:#E1E4E8;">                eids </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> frontier.edge_ids(torch.cat([heads, heads]), torch.cat([tails, neg_tails]), </span><span style="color:#FFAB70;">return_uv</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">                </span></span>
<span class="line"><span style="color:#E1E4E8;">                </span><span style="color:#F97583;">if</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(eids) </span><span style="color:#F97583;">&gt;</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#E1E4E8;">                    old_frontier </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> frontier</span></span>
<span class="line"><span style="color:#E1E4E8;">                    frontier </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> dgl.remove_edges(old_frontier, eids)</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># 只保留seeds这些节点，将frontier压缩成block</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># 并设置block的input/output nodes</span></span>
<span class="line"><span style="color:#E1E4E8;">            block </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> compact_and_copy(frontier, seeds)</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># 本层的输入节点就是下一层的seeds</span></span>
<span class="line"><span style="color:#E1E4E8;">            seeds </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> block.srcdata[dgl.</span><span style="color:#79B8FF;">NID</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">            blocks.insert(</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, block)</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">return</span><span style="color:#E1E4E8;"> blocks</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">NeighborSampler</span><span style="color:#24292E;">(</span><span style="color:#005CC5;">object</span><span style="color:#24292E;">):  </span><span style="color:#6A737D;"># 图卷积的邻居采样</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">(self, g, user_type, item_type, random_walk_length, random_walk_restart_prob,</span></span>
<span class="line"><span style="color:#24292E;">                 num_random_walks, num_neighbors, num_layers):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.g </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> g</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.user_type </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> user_type</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.item_type </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> item_type</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.user_to_item_etype </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">list</span><span style="color:#24292E;">(g.metagraph()[user_type][item_type])[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.item_to_user_etype </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">list</span><span style="color:#24292E;">(g.metagraph()[item_type][user_type])[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#24292E;">        </span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 每层都有一个采样器，根据随机游走来决定某节点邻居的重要性(主要的实现已封装在PinSAGESampler中)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 可以认为经过多次游走，落脚于某邻居节点的次数越多，则这个邻居越重要，就更应该优先作为邻居</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.samplers </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> [</span></span>
<span class="line"><span style="color:#24292E;">            dgl.sampling.PinSAGESampler(g, item_type, user_type, random_walk_length,</span></span>
<span class="line"><span style="color:#24292E;">                random_walk_restart_prob, num_random_walks, num_neighbors)</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> _ </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">range</span><span style="color:#24292E;">(num_layers)]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">sample_blocks</span><span style="color:#24292E;">(self, seeds, heads</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, tails</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, neg_tails</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">None</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#032F62;">&quot;&quot;&quot;根据随机游走得到的重要性权重，进行邻居采样&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#24292E;">        blocks </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> []</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> sampler </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.samplers:</span></span>
<span class="line"><span style="color:#24292E;">            frontier </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sampler(seeds) </span><span style="color:#6A737D;"># 通过随机游走进行重要性采样，生成中间状态</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> heads </span><span style="color:#D73A49;">is</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">None</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#6A737D;"># 如果是在训练，需要将heads-&gt;tails 和 head-&gt;neg_tails这些待预测的边都去掉</span></span>
<span class="line"><span style="color:#24292E;">                eids </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> frontier.edge_ids(torch.cat([heads, heads]), torch.cat([tails, neg_tails]), </span><span style="color:#E36209;">return_uv</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">True</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">                </span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">len</span><span style="color:#24292E;">(eids) </span><span style="color:#D73A49;">&gt;</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">                    old_frontier </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> frontier</span></span>
<span class="line"><span style="color:#24292E;">                    frontier </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> dgl.remove_edges(old_frontier, eids)</span></span>
<span class="line"><span style="color:#24292E;">            </span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 只保留seeds这些节点，将frontier压缩成block</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 并设置block的input/output nodes</span></span>
<span class="line"><span style="color:#24292E;">            block </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> compact_and_copy(frontier, seeds)</span></span>
<span class="line"><span style="color:#24292E;">            </span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 本层的输入节点就是下一层的seeds</span></span>
<span class="line"><span style="color:#24292E;">            seeds </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> block.srcdata[dgl.</span><span style="color:#005CC5;">NID</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#24292E;">            blocks.insert(</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">, block)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> blocks</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br></div></div><p>其次<strong> sample_from_item_pairs</strong> 方法是通过上面得到的 heads, tails, neg_tails 分别构建基于正样本对以及基于负样本对的 item-item 图。由 heads→tails 生成的 pos_graph，用于计算 pairwise loss 中的 pos_score，由 heads→neg_tails 生成的 neg_graph，用于计算 pairwise loss 中的 neg_score。</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#F97583;">class</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">NeighborSampler</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">object</span><span style="color:#E1E4E8;">):  </span><span style="color:#6A737D;"># 图卷积的邻居采样</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(self, g, user_type, item_type, random_walk_length, ....):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">pass</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">sample_blocks</span><span style="color:#E1E4E8;">(self, seeds, heads</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">, tails</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">, neg_tails</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">pass</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">sample_from_item_pairs</span><span style="color:#E1E4E8;">(self, heads, tails, neg_tails):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 由heads-&gt;tails构建positive graph， num_nodes设置成原图中所有item节点</span></span>
<span class="line"><span style="color:#E1E4E8;">        pos_graph </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> dgl.graph(</span></span>
<span class="line"><span style="color:#E1E4E8;">            (heads, tails),</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#FFAB70;">num_nodes</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.g.number_of_nodes(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.item_type))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 由heads-&gt;neg_tails构建negative graph，num_nodes设置成原图中所有item节点</span></span>
<span class="line"><span style="color:#E1E4E8;">        neg_graph </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> dgl.graph(</span></span>
<span class="line"><span style="color:#E1E4E8;">            (heads, neg_tails),</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#FFAB70;">num_nodes</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.g.number_of_nodes(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.item_type))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 去除heads, tails, neg_tails以外的节点，将大图压缩成小图，避免与本轮训练不相关节点的结构也传入模型，提升计算效率</span></span>
<span class="line"><span style="color:#E1E4E8;">        pos_graph, neg_graph </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> dgl.compact_graphs([pos_graph, neg_graph])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 压缩后的图上的节点是原图中的编号</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 注意这时pos_graph与neg_graph不是分开编号的两个图，它们来自于同一幅由heads, tails, neg_tails组成的大图</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># pos_graph和neg_graph中的节点相同，都是heads+tails+neg_tails，即这里的seeds，pos_graph和neg_graph只是边不同而已</span></span>
<span class="line"><span style="color:#E1E4E8;">        seeds </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> pos_graph.ndata[dgl.</span><span style="color:#79B8FF;">NID</span><span style="color:#E1E4E8;">]  </span><span style="color:#6A737D;"># 字典  不同类型节点为一个tensor，为每个节点的id值</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">        blocks </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.sample_blocks(seeds, heads, tails, neg_tails)</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">return</span><span style="color:#E1E4E8;"> pos_graph, neg_graph, blocks</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">NeighborSampler</span><span style="color:#24292E;">(</span><span style="color:#005CC5;">object</span><span style="color:#24292E;">):  </span><span style="color:#6A737D;"># 图卷积的邻居采样</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">(self, g, user_type, item_type, random_walk_length, ....):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">pass</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">sample_blocks</span><span style="color:#24292E;">(self, seeds, heads</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, tails</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, neg_tails</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">None</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">pass</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">sample_from_item_pairs</span><span style="color:#24292E;">(self, heads, tails, neg_tails):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 由heads-&gt;tails构建positive graph， num_nodes设置成原图中所有item节点</span></span>
<span class="line"><span style="color:#24292E;">        pos_graph </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> dgl.graph(</span></span>
<span class="line"><span style="color:#24292E;">            (heads, tails),</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#E36209;">num_nodes</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.g.number_of_nodes(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.item_type))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 由heads-&gt;neg_tails构建negative graph，num_nodes设置成原图中所有item节点</span></span>
<span class="line"><span style="color:#24292E;">        neg_graph </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> dgl.graph(</span></span>
<span class="line"><span style="color:#24292E;">            (heads, neg_tails),</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#E36209;">num_nodes</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.g.number_of_nodes(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.item_type))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 去除heads, tails, neg_tails以外的节点，将大图压缩成小图，避免与本轮训练不相关节点的结构也传入模型，提升计算效率</span></span>
<span class="line"><span style="color:#24292E;">        pos_graph, neg_graph </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> dgl.compact_graphs([pos_graph, neg_graph])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 压缩后的图上的节点是原图中的编号</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 注意这时pos_graph与neg_graph不是分开编号的两个图，它们来自于同一幅由heads, tails, neg_tails组成的大图</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># pos_graph和neg_graph中的节点相同，都是heads+tails+neg_tails，即这里的seeds，pos_graph和neg_graph只是边不同而已</span></span>
<span class="line"><span style="color:#24292E;">        seeds </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> pos_graph.ndata[dgl.</span><span style="color:#005CC5;">NID</span><span style="color:#24292E;">]  </span><span style="color:#6A737D;"># 字典  不同类型节点为一个tensor，为每个节点的id值</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        blocks </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.sample_blocks(seeds, heads, tails, neg_tails)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> pos_graph, neg_graph, blocks</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br></div></div><h3 id="pinsage-1" tabindex="-1">PinSAGE <a class="header-anchor" href="#pinsage-1" aria-label="Permalink to &quot;PinSAGE&quot;">​</a></h3><p>在得到所有所需的数据之后，看看模型结构。其中主要分为三个部分:<strong> 节点特征映射</strong>，** 多层卷积模块 ** 和 <strong>给边打分</strong>。</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#F97583;">class</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">PinSAGEModel</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(self, full_graph, ntype, textsets, hidden_dims, n_layers):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">super</span><span style="color:#E1E4E8;">().</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">()</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 负责将节点上的各种特征都映射成向量，并聚合在一起，形成这个节点的原始特征向量</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.proj </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> layers.LinearProjector(full_graph, ntype, textsets, hidden_dims)</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 负责多层图卷积，得到各节点最终的embedding</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.sage </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> layers.SAGENet(hidden_dims, n_layers)</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 负责根据首尾两端的节点的embedding，计算边上的得分</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.scorer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> layers.ItemToItemScorer(full_graph, ntype)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">forward</span><span style="color:#E1E4E8;">(self, pos_graph, neg_graph, blocks):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#9ECBFF;">&quot;&quot;&quot; pos_graph, neg_graph, blocks 的最后一层都对应batch中 heads+tails+neg_tails 这些节点</span></span>
<span class="line"><span style="color:#9ECBFF;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 得到batch中heads+tails+neg_tails这些节点的最终embedding</span></span>
<span class="line"><span style="color:#E1E4E8;">        h_item </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.get_repr(blocks)</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 得到heads-&gt;tails这些边上的得分</span></span>
<span class="line"><span style="color:#E1E4E8;">        pos_score </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.scorer(pos_graph, h_item)</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 得到heads-&gt;neg_tails这些边上的得分</span></span>
<span class="line"><span style="color:#E1E4E8;">        neg_score </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.scorer(neg_graph, h_item)</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># pos_graph与neg_graph边数相等，因此neg_score与pos_score相减</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 返回margin hinge loss，这里的margin是1 </span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">return</span><span style="color:#E1E4E8;"> (neg_score </span><span style="color:#F97583;">-</span><span style="color:#E1E4E8;"> pos_score </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">).clamp(</span><span style="color:#FFAB70;">min</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">get_repr</span><span style="color:#E1E4E8;">(self, blocks):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#9ECBFF;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#9ECBFF;">        通过self.sage，经过多层卷积，得到输出节点上的卷积结果，再加上这些输出节点上原始特征的映射结果</span></span>
<span class="line"><span style="color:#9ECBFF;">        得到输出节点上最终的向量表示</span></span>
<span class="line"><span style="color:#9ECBFF;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">        h_item </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.proj(blocks[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">].srcdata) </span><span style="color:#6A737D;"># 将输入节点上的原始特征映射成hidden_dims长的向量</span></span>
<span class="line"><span style="color:#E1E4E8;">        h_item_dst </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.proj(blocks[</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">].dstdata) </span><span style="color:#6A737D;"># 将输出节点上的原始特征映射成hidden_dims长的向量</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">return</span><span style="color:#E1E4E8;"> h_item_dst </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.sage(blocks, h_item)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">PinSAGEModel</span><span style="color:#24292E;">(</span><span style="color:#6F42C1;">nn</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">Module</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">(self, full_graph, ntype, textsets, hidden_dims, n_layers):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">super</span><span style="color:#24292E;">().</span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 负责将节点上的各种特征都映射成向量，并聚合在一起，形成这个节点的原始特征向量</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.proj </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> layers.LinearProjector(full_graph, ntype, textsets, hidden_dims)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 负责多层图卷积，得到各节点最终的embedding</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.sage </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> layers.SAGENet(hidden_dims, n_layers)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 负责根据首尾两端的节点的embedding，计算边上的得分</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.scorer </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> layers.ItemToItemScorer(full_graph, ntype)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">forward</span><span style="color:#24292E;">(self, pos_graph, neg_graph, blocks):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#032F62;">&quot;&quot;&quot; pos_graph, neg_graph, blocks 的最后一层都对应batch中 heads+tails+neg_tails 这些节点</span></span>
<span class="line"><span style="color:#032F62;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 得到batch中heads+tails+neg_tails这些节点的最终embedding</span></span>
<span class="line"><span style="color:#24292E;">        h_item </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.get_repr(blocks)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 得到heads-&gt;tails这些边上的得分</span></span>
<span class="line"><span style="color:#24292E;">        pos_score </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.scorer(pos_graph, h_item)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 得到heads-&gt;neg_tails这些边上的得分</span></span>
<span class="line"><span style="color:#24292E;">        neg_score </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.scorer(neg_graph, h_item)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># pos_graph与neg_graph边数相等，因此neg_score与pos_score相减</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 返回margin hinge loss，这里的margin是1 </span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> (neg_score </span><span style="color:#D73A49;">-</span><span style="color:#24292E;"> pos_score </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">).clamp(</span><span style="color:#E36209;">min</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">get_repr</span><span style="color:#24292E;">(self, blocks):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#032F62;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#032F62;">        通过self.sage，经过多层卷积，得到输出节点上的卷积结果，再加上这些输出节点上原始特征的映射结果</span></span>
<span class="line"><span style="color:#032F62;">        得到输出节点上最终的向量表示</span></span>
<span class="line"><span style="color:#032F62;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#24292E;">        h_item </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.proj(blocks[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">].srcdata) </span><span style="color:#6A737D;"># 将输入节点上的原始特征映射成hidden_dims长的向量</span></span>
<span class="line"><span style="color:#24292E;">        h_item_dst </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.proj(blocks[</span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">].dstdata) </span><span style="color:#6A737D;"># 将输出节点上的原始特征映射成hidden_dims长的向量</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> h_item_dst </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.sage(blocks, h_item)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br></div></div><p>** 节点特征映射：** 由于节点使用到了多种类型（int,float array,text）的原始特征，这里使用了一个 DNN 层来融合成固定的长度。</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#F97583;">class</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">LinearProjector</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(self, full_graph, ntype, textset, hidden_dims):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">super</span><span style="color:#E1E4E8;">().</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">()</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.ntype </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> ntype</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 初始化参数,这里为全图中所有节点特征初始化</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 如果特征类型是float，就定义一个nn.Linear线性变化为指定维度</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 如果特征类型是int，就定义Embedding矩阵，将id型特征转化为向量</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.inputs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> _init_input_modules(full_graph, ntype, textset, hidden_dims)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">forward</span><span style="color:#E1E4E8;">(self, ndata):</span></span>
<span class="line"><span style="color:#E1E4E8;">        projections </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> []</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> feature, data </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> ndata.items():</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># NID是计算子图中节点、边在原图中的编号，没必要用做特征</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#F97583;">if</span><span style="color:#E1E4E8;"> feature </span><span style="color:#F97583;">==</span><span style="color:#E1E4E8;"> dgl.</span><span style="color:#79B8FF;">NID</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#E1E4E8;">                </span><span style="color:#F97583;">continue</span></span>
<span class="line"><span style="color:#E1E4E8;">            module </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.inputs[feature]  </span><span style="color:#6A737D;"># 根据特征名取出相应的特征转化器</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;">#  对文本属性进行处理 </span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#F97583;">if</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">isinstance</span><span style="color:#E1E4E8;">(module, (BagOfWords, BagOfWordsPretrained)):</span></span>
<span class="line"><span style="color:#E1E4E8;">                length </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> ndata[feature </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> </span><span style="color:#9ECBFF;">&#39;__len&#39;</span><span style="color:#E1E4E8;">]  </span></span>
<span class="line"><span style="color:#E1E4E8;">                result </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> module(data, length)</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#F97583;">else</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#E1E4E8;">                result </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> module(data)   </span><span style="color:#6A737D;"># look_up</span></span>
<span class="line"><span style="color:#E1E4E8;">            projections.append(result)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 将每个特征都映射后的hidden_dims长的向量，element-wise相加</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">return</span><span style="color:#E1E4E8;"> torch.stack(projections, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">).sum(</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)   </span><span style="color:#6A737D;"># [nodes, hidden_dims]</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">LinearProjector</span><span style="color:#24292E;">(</span><span style="color:#6F42C1;">nn</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">Module</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">(self, full_graph, ntype, textset, hidden_dims):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">super</span><span style="color:#24292E;">().</span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.ntype </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> ntype</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 初始化参数,这里为全图中所有节点特征初始化</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 如果特征类型是float，就定义一个nn.Linear线性变化为指定维度</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 如果特征类型是int，就定义Embedding矩阵，将id型特征转化为向量</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.inputs </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> _init_input_modules(full_graph, ntype, textset, hidden_dims)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">forward</span><span style="color:#24292E;">(self, ndata):</span></span>
<span class="line"><span style="color:#24292E;">        projections </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> []</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> feature, data </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> ndata.items():</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># NID是计算子图中节点、边在原图中的编号，没必要用做特征</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> feature </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> dgl.</span><span style="color:#005CC5;">NID</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">                </span><span style="color:#D73A49;">continue</span></span>
<span class="line"><span style="color:#24292E;">            module </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.inputs[feature]  </span><span style="color:#6A737D;"># 根据特征名取出相应的特征转化器</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;">#  对文本属性进行处理 </span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">isinstance</span><span style="color:#24292E;">(module, (BagOfWords, BagOfWordsPretrained)):</span></span>
<span class="line"><span style="color:#24292E;">                length </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> ndata[feature </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&#39;__len&#39;</span><span style="color:#24292E;">]  </span></span>
<span class="line"><span style="color:#24292E;">                result </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> module(data, length)</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">else</span><span style="color:#24292E;">:</span></span>
<span class="line"><span style="color:#24292E;">                result </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> module(data)   </span><span style="color:#6A737D;"># look_up</span></span>
<span class="line"><span style="color:#24292E;">            projections.append(result)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 将每个特征都映射后的hidden_dims长的向量，element-wise相加</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> torch.stack(projections, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">).sum(</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">)   </span><span style="color:#6A737D;"># [nodes, hidden_dims]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br></div></div><p>** 多层卷积模块：** 根据采样得到的节点 blocks，然后通过进行逐层卷积，得到各节点最终的 embedding。</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#F97583;">class</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">SAGENet</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(self, hidden_dims, n_layers):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#9ECBFF;">&quot;&quot;&quot;g : 二部图&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">super</span><span style="color:#E1E4E8;">().</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">()</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.convs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.ModuleList()</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> _ </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">range</span><span style="color:#E1E4E8;">(n_layers):</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.convs.append(WeightedSAGEConv(hidden_dims, hidden_dims, hidden_dims))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">forward</span><span style="color:#E1E4E8;">(self, blocks, h):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 这里根据邻居节点进逐层聚合</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> layer, block </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">zip</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.convs, blocks):</span></span>
<span class="line"><span style="color:#E1E4E8;">            h_dst </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> h[:block.number_of_nodes(</span><span style="color:#9ECBFF;">&#39;DST/&#39;</span><span style="color:#E1E4E8;"> </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> block.ntypes[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">])]  </span><span style="color:#6A737D;">#前一次卷积的结果</span></span>
<span class="line"><span style="color:#E1E4E8;">            h </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> layer(block, (h, h_dst), block.edata[</span><span style="color:#9ECBFF;">&#39;weights&#39;</span><span style="color:#E1E4E8;">])</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">return</span><span style="color:#E1E4E8;"> h</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">SAGENet</span><span style="color:#24292E;">(</span><span style="color:#6F42C1;">nn</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">Module</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">(self, hidden_dims, n_layers):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#032F62;">&quot;&quot;&quot;g : 二部图&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">super</span><span style="color:#24292E;">().</span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.convs </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.ModuleList()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> _ </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">range</span><span style="color:#24292E;">(n_layers):</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.convs.append(WeightedSAGEConv(hidden_dims, hidden_dims, hidden_dims))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">forward</span><span style="color:#24292E;">(self, blocks, h):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 这里根据邻居节点进逐层聚合</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> layer, block </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">zip</span><span style="color:#24292E;">(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.convs, blocks):</span></span>
<span class="line"><span style="color:#24292E;">            h_dst </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> h[:block.number_of_nodes(</span><span style="color:#032F62;">&#39;DST/&#39;</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> block.ntypes[</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">])]  </span><span style="color:#6A737D;">#前一次卷积的结果</span></span>
<span class="line"><span style="color:#24292E;">            h </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> layer(block, (h, h_dst), block.edata[</span><span style="color:#032F62;">&#39;weights&#39;</span><span style="color:#24292E;">])</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> h</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>其中 WeightedSAGEConv 为根据邻居权重的聚合函数。</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#F97583;">class</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">WeightedSAGEConv</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(self, input_dims, hidden_dims, output_dims, act</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">F.relu):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">super</span><span style="color:#E1E4E8;">().</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">()</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.act </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> act</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.Q </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Linear(input_dims, hidden_dims)</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.W </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Linear(input_dims </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> hidden_dims, output_dims)</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.reset_parameters()</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.dropout </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Dropout(</span><span style="color:#79B8FF;">0.5</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">reset_parameters</span><span style="color:#E1E4E8;">(self):</span></span>
<span class="line"><span style="color:#E1E4E8;">        gain </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.init.calculate_gain(</span><span style="color:#9ECBFF;">&#39;relu&#39;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">        nn.init.xavier_uniform_(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.Q.weight, </span><span style="color:#FFAB70;">gain</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">gain)</span></span>
<span class="line"><span style="color:#E1E4E8;">        nn.init.xavier_uniform_(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.W.weight, </span><span style="color:#FFAB70;">gain</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">gain)</span></span>
<span class="line"><span style="color:#E1E4E8;">        nn.init.constant_(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.Q.bias, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">        nn.init.constant_(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.W.bias, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">forward</span><span style="color:#E1E4E8;">(self, g, h, weights):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#9ECBFF;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#9ECBFF;">        g : 基于batch的子图</span></span>
<span class="line"><span style="color:#9ECBFF;">        h : 节点特征</span></span>
<span class="line"><span style="color:#9ECBFF;">        weights : 边的权重</span></span>
<span class="line"><span style="color:#9ECBFF;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">        h_src, h_dst </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> h  </span><span style="color:#6A737D;"># 邻居节点特征，自身节点特征</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">with</span><span style="color:#E1E4E8;"> g.local_scope():</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># 将src节点上的原始特征映射成hidden_dims长，存储于&#39;n&#39;字段</span></span>
<span class="line"><span style="color:#E1E4E8;">            g.srcdata[</span><span style="color:#9ECBFF;">&#39;n&#39;</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.act(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.Q(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.dropout(h_src)))</span></span>
<span class="line"><span style="color:#E1E4E8;">            g.edata[</span><span style="color:#9ECBFF;">&#39;w&#39;</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> weights.float()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># src节点上的特征&#39;n&#39;乘以边上的权重，构成消息&#39;m&#39;</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># dst节点将所有接收到的消息&#39;m&#39;，相加起来，存入dst节点的&#39;n&#39;字段</span></span>
<span class="line"><span style="color:#E1E4E8;">            g.update_all(fn.u_mul_e(</span><span style="color:#9ECBFF;">&#39;n&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;w&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;m&#39;</span><span style="color:#E1E4E8;">), fn.sum(</span><span style="color:#9ECBFF;">&#39;m&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;n&#39;</span><span style="color:#E1E4E8;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># 将边上的权重w拷贝成消息&#39;m&#39;</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># dst节点将所有接收到的消息&#39;m&#39;，相加起来，存入dst节点的&#39;ws&#39;字段</span></span>
<span class="line"><span style="color:#E1E4E8;">            g.update_all(fn.copy_e(</span><span style="color:#9ECBFF;">&#39;w&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;m&#39;</span><span style="color:#E1E4E8;">), fn.sum(</span><span style="color:#9ECBFF;">&#39;m&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;ws&#39;</span><span style="color:#E1E4E8;">))</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># 邻居节点的embedding的加权和</span></span>
<span class="line"><span style="color:#E1E4E8;">            n </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> g.dstdata[</span><span style="color:#9ECBFF;">&#39;n&#39;</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">            ws </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> g.dstdata[</span><span style="color:#9ECBFF;">&#39;ws&#39;</span><span style="color:#E1E4E8;">].unsqueeze(</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">).clamp(</span><span style="color:#FFAB70;">min</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)  </span><span style="color:#6A737D;"># 边上权重之和</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># 先将邻居节点的embedding，做加权平均</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># 再拼接上一轮卷积后，dst节点自身的embedding</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># 再经过线性变化与非线性激活，得到这一轮卷积后各dst节点的embedding</span></span>
<span class="line"><span style="color:#E1E4E8;">            z </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.act(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.W(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.dropout(torch.cat([n </span><span style="color:#F97583;">/</span><span style="color:#E1E4E8;"> ws, h_dst], </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">))))</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># 本轮卷积后，各dst节点的embedding除以模长，进行归一化</span></span>
<span class="line"><span style="color:#E1E4E8;">            z_norm </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> z.norm(</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">keepdim</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">            z_norm </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.where(z_norm </span><span style="color:#F97583;">==</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, torch.tensor(</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">.).to(z_norm), z_norm)</span></span>
<span class="line"><span style="color:#E1E4E8;">            z </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> z </span><span style="color:#F97583;">/</span><span style="color:#E1E4E8;"> z_norm</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#F97583;">return</span><span style="color:#E1E4E8;"> z</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">WeightedSAGEConv</span><span style="color:#24292E;">(</span><span style="color:#6F42C1;">nn</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">Module</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">(self, input_dims, hidden_dims, output_dims, act</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">F.relu):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">super</span><span style="color:#24292E;">().</span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.act </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> act</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.Q </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.Linear(input_dims, hidden_dims)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.W </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.Linear(input_dims </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> hidden_dims, output_dims)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.reset_parameters()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.dropout </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.Dropout(</span><span style="color:#005CC5;">0.5</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">reset_parameters</span><span style="color:#24292E;">(self):</span></span>
<span class="line"><span style="color:#24292E;">        gain </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.init.calculate_gain(</span><span style="color:#032F62;">&#39;relu&#39;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">        nn.init.xavier_uniform_(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.Q.weight, </span><span style="color:#E36209;">gain</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">gain)</span></span>
<span class="line"><span style="color:#24292E;">        nn.init.xavier_uniform_(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.W.weight, </span><span style="color:#E36209;">gain</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">gain)</span></span>
<span class="line"><span style="color:#24292E;">        nn.init.constant_(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.Q.bias, </span><span style="color:#005CC5;">0</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">        nn.init.constant_(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.W.bias, </span><span style="color:#005CC5;">0</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">forward</span><span style="color:#24292E;">(self, g, h, weights):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#032F62;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#032F62;">        g : 基于batch的子图</span></span>
<span class="line"><span style="color:#032F62;">        h : 节点特征</span></span>
<span class="line"><span style="color:#032F62;">        weights : 边的权重</span></span>
<span class="line"><span style="color:#032F62;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#24292E;">        h_src, h_dst </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> h  </span><span style="color:#6A737D;"># 邻居节点特征，自身节点特征</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">with</span><span style="color:#24292E;"> g.local_scope():</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 将src节点上的原始特征映射成hidden_dims长，存储于&#39;n&#39;字段</span></span>
<span class="line"><span style="color:#24292E;">            g.srcdata[</span><span style="color:#032F62;">&#39;n&#39;</span><span style="color:#24292E;">] </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.act(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.Q(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.dropout(h_src)))</span></span>
<span class="line"><span style="color:#24292E;">            g.edata[</span><span style="color:#032F62;">&#39;w&#39;</span><span style="color:#24292E;">] </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> weights.float()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># src节点上的特征&#39;n&#39;乘以边上的权重，构成消息&#39;m&#39;</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># dst节点将所有接收到的消息&#39;m&#39;，相加起来，存入dst节点的&#39;n&#39;字段</span></span>
<span class="line"><span style="color:#24292E;">            g.update_all(fn.u_mul_e(</span><span style="color:#032F62;">&#39;n&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;w&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;m&#39;</span><span style="color:#24292E;">), fn.sum(</span><span style="color:#032F62;">&#39;m&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;n&#39;</span><span style="color:#24292E;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 将边上的权重w拷贝成消息&#39;m&#39;</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># dst节点将所有接收到的消息&#39;m&#39;，相加起来，存入dst节点的&#39;ws&#39;字段</span></span>
<span class="line"><span style="color:#24292E;">            g.update_all(fn.copy_e(</span><span style="color:#032F62;">&#39;w&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;m&#39;</span><span style="color:#24292E;">), fn.sum(</span><span style="color:#032F62;">&#39;m&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;ws&#39;</span><span style="color:#24292E;">))</span></span>
<span class="line"><span style="color:#24292E;">            </span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 邻居节点的embedding的加权和</span></span>
<span class="line"><span style="color:#24292E;">            n </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> g.dstdata[</span><span style="color:#032F62;">&#39;n&#39;</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#24292E;">            ws </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> g.dstdata[</span><span style="color:#032F62;">&#39;ws&#39;</span><span style="color:#24292E;">].unsqueeze(</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">).clamp(</span><span style="color:#E36209;">min</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">)  </span><span style="color:#6A737D;"># 边上权重之和</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 先将邻居节点的embedding，做加权平均</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 再拼接上一轮卷积后，dst节点自身的embedding</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 再经过线性变化与非线性激活，得到这一轮卷积后各dst节点的embedding</span></span>
<span class="line"><span style="color:#24292E;">            z </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.act(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.W(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.dropout(torch.cat([n </span><span style="color:#D73A49;">/</span><span style="color:#24292E;"> ws, h_dst], </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">))))</span></span>
<span class="line"><span style="color:#24292E;">            </span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 本轮卷积后，各dst节点的embedding除以模长，进行归一化</span></span>
<span class="line"><span style="color:#24292E;">            z_norm </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> z.norm(</span><span style="color:#005CC5;">2</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#E36209;">keepdim</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">True</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">            z_norm </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.where(z_norm </span><span style="color:#D73A49;">==</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">0</span><span style="color:#24292E;">, torch.tensor(</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">.).to(z_norm), z_norm)</span></span>
<span class="line"><span style="color:#24292E;">            z </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> z </span><span style="color:#D73A49;">/</span><span style="color:#24292E;"> z_norm</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> z</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br></div></div><p><strong>给边打分：</strong> 经过 SAGENet 得到了 batch 内所有节点的 embedding，这时需要根据学习到的 embedding 为 pos_graph 和 neg_graph 中的每个边打分，即计算正样本对和负样本的內积。具体逻辑是根据两端节点 embedding 的点积，然后加上两端节点的 bias。</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#F97583;">class</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">ItemToItemScorer</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(self, full_graph, ntype):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">super</span><span style="color:#E1E4E8;">().</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">()</span></span>
<span class="line"><span style="color:#E1E4E8;">        n_nodes </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> full_graph.number_of_nodes(ntype)</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.bias </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Parameter(torch.zeros(n_nodes, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">_add_bias</span><span style="color:#E1E4E8;">(self, edges):</span></span>
<span class="line"><span style="color:#E1E4E8;">        bias_src </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.bias[edges.src[dgl.</span><span style="color:#79B8FF;">NID</span><span style="color:#E1E4E8;">]]</span></span>
<span class="line"><span style="color:#E1E4E8;">        bias_dst </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.bias[edges.dst[dgl.</span><span style="color:#79B8FF;">NID</span><span style="color:#E1E4E8;">]]</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 边上两顶点的embedding的点积，再加上两端节点的bias</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">return</span><span style="color:#E1E4E8;"> {</span><span style="color:#9ECBFF;">&#39;s&#39;</span><span style="color:#E1E4E8;">: edges.data[</span><span style="color:#9ECBFF;">&#39;s&#39;</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> bias_src </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> bias_dst}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">forward</span><span style="color:#E1E4E8;">(self, item_item_graph, h):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#9ECBFF;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#9ECBFF;">        item_item_graph : 每个边  为 pair 对</span></span>
<span class="line"><span style="color:#9ECBFF;">        h : 每个节点隐层状态</span></span>
<span class="line"><span style="color:#9ECBFF;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">with</span><span style="color:#E1E4E8;"> item_item_graph.local_scope():</span></span>
<span class="line"><span style="color:#E1E4E8;">            item_item_graph.ndata[</span><span style="color:#9ECBFF;">&#39;h&#39;</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> h</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># 边两端节点的embedding做点积，保存到s</span></span>
<span class="line"><span style="color:#E1E4E8;">            item_item_graph.apply_edges(fn.u_dot_v(</span><span style="color:#9ECBFF;">&#39;h&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;h&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;s&#39;</span><span style="color:#E1E4E8;">))</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># 为每个边加上偏置，即加上两个顶点的偏置</span></span>
<span class="line"><span style="color:#E1E4E8;">            item_item_graph.apply_edges(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">._add_bias)</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#6A737D;"># 算出来的得分为 pair 的预测得分</span></span>
<span class="line"><span style="color:#E1E4E8;">            pair_score </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> item_item_graph.edata[</span><span style="color:#9ECBFF;">&#39;s&#39;</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">return</span><span style="color:#E1E4E8;"> pair_score</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">ItemToItemScorer</span><span style="color:#24292E;">(</span><span style="color:#6F42C1;">nn</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">Module</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">(self, full_graph, ntype):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">super</span><span style="color:#24292E;">().</span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">()</span></span>
<span class="line"><span style="color:#24292E;">        n_nodes </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> full_graph.number_of_nodes(ntype)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.bias </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> nn.Parameter(torch.zeros(n_nodes, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">_add_bias</span><span style="color:#24292E;">(self, edges):</span></span>
<span class="line"><span style="color:#24292E;">        bias_src </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.bias[edges.src[dgl.</span><span style="color:#005CC5;">NID</span><span style="color:#24292E;">]]</span></span>
<span class="line"><span style="color:#24292E;">        bias_dst </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.bias[edges.dst[dgl.</span><span style="color:#005CC5;">NID</span><span style="color:#24292E;">]]</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 边上两顶点的embedding的点积，再加上两端节点的bias</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> {</span><span style="color:#032F62;">&#39;s&#39;</span><span style="color:#24292E;">: edges.data[</span><span style="color:#032F62;">&#39;s&#39;</span><span style="color:#24292E;">] </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> bias_src </span><span style="color:#D73A49;">+</span><span style="color:#24292E;"> bias_dst}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">forward</span><span style="color:#24292E;">(self, item_item_graph, h):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#032F62;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#032F62;">        item_item_graph : 每个边  为 pair 对</span></span>
<span class="line"><span style="color:#032F62;">        h : 每个节点隐层状态</span></span>
<span class="line"><span style="color:#032F62;">        &quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">with</span><span style="color:#24292E;"> item_item_graph.local_scope():</span></span>
<span class="line"><span style="color:#24292E;">            item_item_graph.ndata[</span><span style="color:#032F62;">&#39;h&#39;</span><span style="color:#24292E;">] </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> h</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 边两端节点的embedding做点积，保存到s</span></span>
<span class="line"><span style="color:#24292E;">            item_item_graph.apply_edges(fn.u_dot_v(</span><span style="color:#032F62;">&#39;h&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;h&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39;s&#39;</span><span style="color:#24292E;">))</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 为每个边加上偏置，即加上两个顶点的偏置</span></span>
<span class="line"><span style="color:#24292E;">            item_item_graph.apply_edges(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">._add_bias)</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#6A737D;"># 算出来的得分为 pair 的预测得分</span></span>
<span class="line"><span style="color:#24292E;">            pair_score </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> item_item_graph.edata[</span><span style="color:#032F62;">&#39;s&#39;</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> pair_score</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br></div></div><h3 id="训练过程-1" tabindex="-1">训练过程 <a class="header-anchor" href="#训练过程-1" aria-label="Permalink to &quot;训练过程&quot;">​</a></h3><p>介绍完 “数据处理” 和 “PinSAGE 模块” 之后，接下来就是通过训练过程将上述两部分串起来，详细的见代码：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">train</span><span style="color:#E1E4E8;">(dataset, args):</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#6A737D;">#从dataset中加载数据和原图</span></span>
<span class="line"><span style="color:#E1E4E8;">    g </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> dataset[</span><span style="color:#9ECBFF;">&#39;train-graph&#39;</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">	</span><span style="color:#79B8FF;">...</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span></span>
<span class="line"><span style="color:#E1E4E8;">    device </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.device(args.device)</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#6A737D;"># 为节点随机初始化一个id，用于做embedding</span></span>
<span class="line"><span style="color:#E1E4E8;">    g.nodes[user_ntype].data[</span><span style="color:#9ECBFF;">&#39;id&#39;</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.arange(g.number_of_nodes(user_ntype))</span></span>
<span class="line"><span style="color:#E1E4E8;">    g.nodes[item_ntype].data[</span><span style="color:#9ECBFF;">&#39;id&#39;</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.arange(g.number_of_nodes(item_ntype))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    </span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#6A737D;"># 负责采样出batch_size大小的节点列表: heads, tails,  neg_tails</span></span>
<span class="line"><span style="color:#E1E4E8;">    batch_sampler </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> sampler_module.ItemToItemBatchSampler(</span></span>
<span class="line"><span style="color:#E1E4E8;">        g, user_ntype, item_ntype, args.batch_size)</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#6A737D;"># 由一个batch中的heads,tails,neg_tails构建训练这个batch所需要的</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#6A737D;"># pos_graph,neg_graph 和 blocks</span></span>
<span class="line"><span style="color:#E1E4E8;">    neighbor_sampler </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> sampler_module.NeighborSampler(</span></span>
<span class="line"><span style="color:#E1E4E8;">        g, user_ntype, item_ntype, args.random_walk_length,</span></span>
<span class="line"><span style="color:#E1E4E8;">        args.random_walk_restart_prob, args.num_random_walks, args.num_neighbors,</span></span>
<span class="line"><span style="color:#E1E4E8;">        args.num_layers)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#6A737D;"># 每次next()返回: pos_graph,neg_graph和blocks，做训练之用</span></span>
<span class="line"><span style="color:#E1E4E8;">    collator </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> sampler_module.PinSAGECollator(neighbor_sampler, g, item_ntype, textset)</span></span>
<span class="line"><span style="color:#E1E4E8;">    dataloader </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> DataLoader(</span></span>
<span class="line"><span style="color:#E1E4E8;">        batch_sampler,</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#FFAB70;">collate_fn</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">collator.collate_train,</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#FFAB70;">num_workers</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">args.num_workers)</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#6A737D;"># 每次next()返回blocks，做训练中测试之用</span></span>
<span class="line"><span style="color:#E1E4E8;">    dataloader_test </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> DataLoader(</span></span>
<span class="line"><span style="color:#E1E4E8;">        torch.arange(g.number_of_nodes(item_ntype)),</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#FFAB70;">batch_size</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">args.batch_size,</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#FFAB70;">collate_fn</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">collator.collate_test,</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#FFAB70;">num_workers</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">args.num_workers)</span></span>
<span class="line"><span style="color:#E1E4E8;">    dataloader_it </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">iter</span><span style="color:#E1E4E8;">(dataloader)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#6A737D;"># 准备模型</span></span>
<span class="line"><span style="color:#E1E4E8;">    model </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> PinSAGEModel(g, item_ntype, textset, args.hidden_dims, args.num_layers).to(device)</span></span>
<span class="line"><span style="color:#E1E4E8;">    opt </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.optim.Adam(model.parameters(), </span><span style="color:#FFAB70;">lr</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">args.lr)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#6A737D;"># 训练过程</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> epoch_id </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">range</span><span style="color:#E1E4E8;">(args.num_epochs):</span></span>
<span class="line"><span style="color:#E1E4E8;">        model.train()</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> batch_id </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> tqdm.trange(args.batches_per_epoch):</span></span>
<span class="line"><span style="color:#E1E4E8;">            pos_graph, neg_graph, blocks </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">next</span><span style="color:#E1E4E8;">(dataloader_it)</span></span>
<span class="line"><span style="color:#E1E4E8;">            </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> i </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">range</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(blocks)):</span></span>
<span class="line"><span style="color:#E1E4E8;">                blocks[i] </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> blocks[i].to(device)</span></span>
<span class="line"><span style="color:#E1E4E8;">            pos_graph </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> pos_graph.to(device)</span></span>
<span class="line"><span style="color:#E1E4E8;">            neg_graph </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> neg_graph.to(device)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">            loss </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> model(pos_graph, neg_graph, blocks).mean()</span></span>
<span class="line"><span style="color:#E1E4E8;">            opt.zero_grad()</span></span>
<span class="line"><span style="color:#E1E4E8;">            loss.backward()</span></span>
<span class="line"><span style="color:#E1E4E8;">            opt.step()</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">train</span><span style="color:#24292E;">(dataset, args):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;">#从dataset中加载数据和原图</span></span>
<span class="line"><span style="color:#24292E;">    g </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> dataset[</span><span style="color:#032F62;">&#39;train-graph&#39;</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#24292E;">	</span><span style="color:#005CC5;">...</span></span>
<span class="line"><span style="color:#24292E;">    </span></span>
<span class="line"><span style="color:#24292E;">    device </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.device(args.device)</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># 为节点随机初始化一个id，用于做embedding</span></span>
<span class="line"><span style="color:#24292E;">    g.nodes[user_ntype].data[</span><span style="color:#032F62;">&#39;id&#39;</span><span style="color:#24292E;">] </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.arange(g.number_of_nodes(user_ntype))</span></span>
<span class="line"><span style="color:#24292E;">    g.nodes[item_ntype].data[</span><span style="color:#032F62;">&#39;id&#39;</span><span style="color:#24292E;">] </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.arange(g.number_of_nodes(item_ntype))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># 负责采样出batch_size大小的节点列表: heads, tails,  neg_tails</span></span>
<span class="line"><span style="color:#24292E;">    batch_sampler </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sampler_module.ItemToItemBatchSampler(</span></span>
<span class="line"><span style="color:#24292E;">        g, user_ntype, item_ntype, args.batch_size)</span></span>
<span class="line"><span style="color:#24292E;">    </span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># 由一个batch中的heads,tails,neg_tails构建训练这个batch所需要的</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># pos_graph,neg_graph 和 blocks</span></span>
<span class="line"><span style="color:#24292E;">    neighbor_sampler </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sampler_module.NeighborSampler(</span></span>
<span class="line"><span style="color:#24292E;">        g, user_ntype, item_ntype, args.random_walk_length,</span></span>
<span class="line"><span style="color:#24292E;">        args.random_walk_restart_prob, args.num_random_walks, args.num_neighbors,</span></span>
<span class="line"><span style="color:#24292E;">        args.num_layers)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># 每次next()返回: pos_graph,neg_graph和blocks，做训练之用</span></span>
<span class="line"><span style="color:#24292E;">    collator </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> sampler_module.PinSAGECollator(neighbor_sampler, g, item_ntype, textset)</span></span>
<span class="line"><span style="color:#24292E;">    dataloader </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> DataLoader(</span></span>
<span class="line"><span style="color:#24292E;">        batch_sampler,</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#E36209;">collate_fn</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">collator.collate_train,</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#E36209;">num_workers</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">args.num_workers)</span></span>
<span class="line"><span style="color:#24292E;">    </span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># 每次next()返回blocks，做训练中测试之用</span></span>
<span class="line"><span style="color:#24292E;">    dataloader_test </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> DataLoader(</span></span>
<span class="line"><span style="color:#24292E;">        torch.arange(g.number_of_nodes(item_ntype)),</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#E36209;">batch_size</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">args.batch_size,</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#E36209;">collate_fn</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">collator.collate_test,</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#E36209;">num_workers</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">args.num_workers)</span></span>
<span class="line"><span style="color:#24292E;">    dataloader_it </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">iter</span><span style="color:#24292E;">(dataloader)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># 准备模型</span></span>
<span class="line"><span style="color:#24292E;">    model </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> PinSAGEModel(g, item_ntype, textset, args.hidden_dims, args.num_layers).to(device)</span></span>
<span class="line"><span style="color:#24292E;">    opt </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.optim.Adam(model.parameters(), </span><span style="color:#E36209;">lr</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">args.lr)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># 训练过程</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> epoch_id </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">range</span><span style="color:#24292E;">(args.num_epochs):</span></span>
<span class="line"><span style="color:#24292E;">        model.train()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> batch_id </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> tqdm.trange(args.batches_per_epoch):</span></span>
<span class="line"><span style="color:#24292E;">            pos_graph, neg_graph, blocks </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">next</span><span style="color:#24292E;">(dataloader_it)</span></span>
<span class="line"><span style="color:#24292E;">            </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> i </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">range</span><span style="color:#24292E;">(</span><span style="color:#005CC5;">len</span><span style="color:#24292E;">(blocks)):</span></span>
<span class="line"><span style="color:#24292E;">                blocks[i] </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> blocks[i].to(device)</span></span>
<span class="line"><span style="color:#24292E;">            pos_graph </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> pos_graph.to(device)</span></span>
<span class="line"><span style="color:#24292E;">            neg_graph </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> neg_graph.to(device)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">            loss </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> model(pos_graph, neg_graph, blocks).mean()</span></span>
<span class="line"><span style="color:#24292E;">            opt.zero_grad()</span></span>
<span class="line"><span style="color:#24292E;">            loss.backward()</span></span>
<span class="line"><span style="color:#24292E;">            opt.step()</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br></div></div><p>至此，DGL PinSAGE example 的主要实现代码已经全部介绍完了，感兴趣的可以去官网对照源代码自行学习。</p><h2 id="参考" tabindex="-1">参考 <a class="header-anchor" href="#参考" aria-label="Permalink to &quot;参考&quot;">​</a></h2><p><a href="https://arxiv.org/abs/1806.01973" target="_blank" rel="noreferrer">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</a></p><p><a href="https://zhuanlan.zhihu.com/p/275942839" target="_blank" rel="noreferrer">PinSAGE 召回模型及源码分析 (1): PinSAGE 简介</a></p><p><a href="https://zhuanlan.zhihu.com/p/133739758" target="_blank" rel="noreferrer">全面理解 PinSage</a></p><p><a href="https://zhuanlan.zhihu.com/p/461720302" target="_blank" rel="noreferrer">[论文笔记] PinSAGE——Graph Convolutional Neural Networks for Web-Scale Recommender Systems</a></p>`,59);function zn(In,Rn,Jn,On,Wn,Un){return l(),p("div",null,[t,s("ul",null,[s("li",null,[s("mjx-container",r,[(l(),p("svg",c,E)),y]),n(" 表示图上节点的初始化表示，等同于节点自身的特征。")]),s("li",null,[s("mjx-container",d,[(l(),p("svg",m,T)),h]),n(" 表示第 k 层卷积后的节点表示，其来源于两个部分： "),s("ul",null,[s("li",null,[n("第一部分来源于节点 v 的邻居节点集合"),s("mjx-container",_,[(l(),p("svg",u,b)),F]),n("，利用邻居节点的第 k-1 层卷积后的特征"),s("mjx-container",w,[(l(),p("svg",x,C)),A]),n(" 进行 （ "),s("mjx-container",D,[(l(),p("svg",k,B)),H]),n(" ）后，在进行线性变换。这里"),L,n("。")]),s("li",null,[n("第二部分来源于节点 v 的第 k-1 成卷积后的特征"),s("mjx-container",M,[(l(),p("svg",S,P)),j]),n("，进行线性变换。总的来说图卷积的思想是"),q])])]),s("li",null,[n("最后一次卷积结果作为节点的最终表示"),s("mjx-container",V,[(l(),p("svg",Z,z)),I]),n("，以用于下游任务 (节点分类，链路预测或节点召回)。")])]),R,J,O,W,s("p",null,[n("通过上面的公式可以知道，得到节点的表示主要依赖于两部分，其中一部分其邻居节点。因此对于 GraphSAGE 的关键主要分为两步：Sample 采样和 Aggregate 聚合。其中 Sample 的作用是从庞大的邻居节点中选出用于聚合的邻居节点集合"),s("mjx-container",U,[(l(),p("svg",X,K)),Y]),n(" 以达到降低迭代计算复杂度，而聚合操作就是如何利用邻居节点的表示来更新节点 v 的表示，已达到聚合作用。具体的过程如下伪代码所示：")]),ss,ns,as,s("p",null,[n("GraphSAGE 的具体采样过程是，首先根据中心节点集合"),s("mjx-container",ls,[(l(),p("svg",ps,os)),ts]),n("，对集合中每个中心节点通过随机采样的方式对其邻居节点采样固定数量 S 个 (如果邻居节点数量大于 S，采用无放回抽样；如果小于 S，则采用有放回抽样)，形成的集合表示为"),s("mjx-container",rs,[(l(),p("svg",cs,Es)),ys]),n("；以此类推每次都是为前一个得到的集合的每个节点随机采样 S 个邻居，最终得到第 k 层的所有需要参与计算的节点集合"),s("mjx-container",ds,[(l(),p("svg",ms,Ts)),hs]),n("。值得注意的有两点："),_s,n(),s("strong",null,[n("为什么第 k 层所采样的节点集合表示为"),s("mjx-container",us,[(l(),p("svg",gs,Fs)),ws]),n("？")])]),xs,s("p",null,[n("第 k 层所采样的节点集合表示为"),s("mjx-container",fs,[(l(),p("svg",Cs,Ds)),ks]),n(" 主要是因为：采样和聚合过程是相反的，即采样时我们是从中心节点组层进行采样，而聚合的过程是从中心节点的第 k 阶邻居逐层聚合得到前一层的节点表示。因此可以认为聚合阶段是：将 k 阶邻居的信息聚合到 k-1 阶邻居上，k-1 阶邻居的信息聚合到 k-2 阶邻居上，....，1 阶邻居的信息聚合到中心节点上的过程。")]),vs,Bs,s("ul",null,[s("li",null,[s("p",null,[n("Mean 聚合：首先会对邻居节点按照"),Hs,n(" 进行均值聚合，然后将当前节点 k-1 层得到特征"),s("mjx-container",Ls,[(l(),p("svg",Ms,Gs)),Ps]),n(" 与邻居节点均值聚合后的特征 "),s("mjx-container",js,[(l(),p("svg",qs,Zs)),Ns]),zs,n("送入全连接网络后"),Is,n("得到结果。")])]),Rs]),Js,s("ol",null,[s("li",null,[n("聚合邻居： 先将所有的邻居节点经过一次非线性转化 (一层 DNN)，再由聚合函数 (Pooling 聚合) "),s("mjx-container",Os,[(l(),p("svg",Ws,Xs)),$s]),n("（如元素平均，"),Ks,n("等）将所有邻居信息聚合成目标节点的 embedding。这里的加权聚合采用的是通过 random-walk 得到的重要性权重。")]),s("li",null,[n("更新当前节点的 embedding：将目标节点当前的向量 "),s("mjx-container",Ys,[(l(),p("svg",sn,an)),ln]),n(" 与步骤 1 中聚合得到的邻居向量 "),s("mjx-container",pn,[(l(),p("svg",en,tn)),rn]),n(" 进行拼接，在通过一次非线性转化。")]),s("li",null,[n("归一化操作：对目标节点向量 "),s("mjx-container",cn,[(l(),p("svg",En,dn)),mn]),n(" 归一化。")])]),s("p",null,[n("Convolve 算法的聚合方法与 GraphSAGE 的 Pooling 聚合函数相同，主要区别在于对更新得到的向量 "),s("mjx-container",Qn,[(l(),p("svg",Tn,_n)),un]),n(" 进行归一化操作，"),gn]),bn,s("p",null,[n("其中"),s("mjx-container",Fn,[(l(),p("svg",wn,fn)),Cn]),n(" 是学习得到的目标节点 embedding，"),s("mjx-container",An,[(l(),p("svg",Dn,vn)),Bn]),n(" 是与目标节点相关 item 的 embedding，"),s("mjx-container",Hn,[(l(),p("svg",Ln,Sn)),Gn]),n(" 是与目标节点不相关 item 的 embedding，"),s("mjx-container",Pn,[(l(),p("svg",jn,Vn)),Zn]),n(" 为 margin 值，具体大小需要调参。那么对于相关节点 i，以及不相关节点 nk，具体都是如何定义的，这对于召回模型的训练意义重大，让我们看看具体是如何定义的。")]),Nn])}const Kn=e(o,[["render",zn]]);export{$n as __pageData,Kn as default};
