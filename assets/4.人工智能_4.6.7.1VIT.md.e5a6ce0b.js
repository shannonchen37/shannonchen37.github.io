import{_ as a,I as t,o as r,c as i,x as o,U as d}from"./chunks/framework.489e5108.js";const u=JSON.parse('{"title":"VIT","description":"","frontmatter":{},"headers":[],"relativePath":"4.人工智能/4.6.7.1VIT.md","filePath":"4.人工智能/4.6.7.1VIT.md","lastUpdated":1696176798000}'),n={name:"4.人工智能/4.6.7.1VIT.md"},l=d('<h1 id="vit" tabindex="-1">VIT <a class="header-anchor" href="#vit" aria-label="Permalink to &quot;VIT&quot;">​</a></h1><h2 id="前言" tabindex="-1">前言 <a class="header-anchor" href="#前言" aria-label="Permalink to &quot;前言&quot;">​</a></h2><p>VIT 前 Transformer 模型被大量应用在 NLP 自然语言处理当中，而在 CV 领域，Transformer 的注意力机制 attention 也被广泛应用，比如 Se 模块，CBAM 模块等等注意力模块，这些注意力模块能够帮助提升网络性能。</p><p>而<strong> VIT 的工作展示了不需要依赖 CNN 的结构，也可以在图像分类任务上达到很好的效果</strong>。</p><p>同时 VIT 也影响了近 2 年的 CV 领域，改变了自 2012 年 AlexNet 提出以来卷积神经网络在 CV 领域的绝对统治地位。</p><p>在本节内容中我们会带你了解这一框架。</p><h2 id="论文" tabindex="-1">论文 <a class="header-anchor" href="#论文" aria-label="Permalink to &quot;论文&quot;">​</a></h2><p><a href="https://zhuanlan.zhihu.com/p/356155277" target="_blank" rel="noreferrer">知乎</a><a href="https://arxiv.org/abs/2010.11929" target="_blank" rel="noreferrer">论文</a></p><h2 id="模型详解" tabindex="-1">模型详解 <a class="header-anchor" href="#模型详解" aria-label="Permalink to &quot;模型详解&quot;">​</a></h2><p><img src="https://cdn.xyxsw.site/boxcn1wqKtwBc6MCJDm7ehvhXac.png" alt=""></p><h3 id="模型主题结构" tabindex="-1">模型主题结构 <a class="header-anchor" href="#模型主题结构" aria-label="Permalink to &quot;模型主题结构&quot;">​</a></h3><p>结构上，VIT 采取的是原始 Transformer 模型，方便开箱即用，即在 encoder-decoder 结构上与 NLP 的 Transform 模型并无差别。</p><p>主要做出的贡献在于<strong>数据处理和分类头</strong></p><h3 id="patch-embedding" tabindex="-1">Patch embedding <a class="header-anchor" href="#patch-embedding" aria-label="Permalink to &quot;Patch embedding&quot;">​</a></h3><h4 id="从-word-embedding-到-patch-embedding" tabindex="-1">从 Word embedding 到 Patch embedding <a class="header-anchor" href="#从-word-embedding-到-patch-embedding" aria-label="Permalink to &quot;从 Word embedding 到 Patch embedding&quot;">​</a></h4><h5 id="word-embedding" tabindex="-1">Word embedding <a class="header-anchor" href="#word-embedding" aria-label="Permalink to &quot;Word embedding&quot;">​</a></h5><p>简单来说就是用特殊的向量来表示一个句子中的某个词</p><p>即例如</p><blockquote><p>今天天气不错，我要去看电影</p></blockquote><p>其中<strong>我</strong>则编码为 [0.5，0.6，0.6]</p><p>而具体来说 Word embedding 分为以下两步</p><ol><li>对 context 进行分词操作。</li><li>对分好的词进行 one-hot 编码，根据学习相应的权重对 one-hot 编码进行 N（embedded_dim）维空间的映射.</li></ol><h5 id="patch-embedding-1" tabindex="-1">Patch embedding <a class="header-anchor" href="#patch-embedding-1" aria-label="Permalink to &quot;Patch embedding&quot;">​</a></h5><p>简单来说 用一个特殊的向量来表示一张图片中某块图</p><p>例如</p><p><img src="https://cdn.xyxsw.site/boxcn1szLG4Y4s0UkY3kkW18Xoc.png" alt=""></p><p><img src="https://cdn.xyxsw.site/boxcnv2inISAGi2xOauc3pxKpCb.png" alt=""></p><p>其中该张图片的编码为 [0.5，0.6，0.3，....]</p><p>具体来说</p><ol><li>先对图片作分块 <ol><li>假设原始输入的图片数据是 H * W * C,</li><li>假设每个块的长宽为 (P, P)，那么分块的数目为 N=H ∗ W / (P ∗ P)</li><li>其中 vit 的分块是定下每一块的大小然后块的数量为计算结果</li></ol></li><li>然后对每个图片块展平成一维向量 <ol><li>每个向量大小为 P * P * C</li></ol></li><li>接着对每个向量都做一个线性变换（即全连接层），得到 patch embedding</li></ol><h2 id="视频" tabindex="-1">视频 <a class="header-anchor" href="#视频" aria-label="Permalink to &quot;视频&quot;">​</a></h2>',31);function h(c,s,p,b,m,g){const e=t("Bilibili");return r(),i("div",null,[l,o(e,{bvid:"BV15P4y137jb"})])}const P=a(n,[["render",h]]);export{u as __pageData,P as default};
