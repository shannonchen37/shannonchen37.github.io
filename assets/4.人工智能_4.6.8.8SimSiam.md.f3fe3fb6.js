import{_ as a,o as t,c as e,U as i}from"./chunks/framework.489e5108.js";const S=JSON.parse('{"title":"SimSiam","description":"","frontmatter":{},"headers":[],"relativePath":"4.人工智能/4.6.8.8SimSiam.md","filePath":"4.人工智能/4.6.8.8SimSiam.md","lastUpdated":1696176798000}'),r={name:"4.人工智能/4.6.8.8SimSiam.md"},o=i(`<h1 id="simsiam" tabindex="-1">SimSiam <a class="header-anchor" href="#simsiam" aria-label="Permalink to &quot;SimSiam&quot;">​</a></h1><h1 id="前言" tabindex="-1">前言 <a class="header-anchor" href="#前言" aria-label="Permalink to &quot;前言&quot;">​</a></h1><h2 id="提出背景" tabindex="-1">提出背景 <a class="header-anchor" href="#提出背景" aria-label="Permalink to &quot;提出背景&quot;">​</a></h2><p>BYOL 之后，大家都发现对比学习是靠许许多多的小 trick 和技术堆叠起来的，每个技术都有贡献，但是不算很多，前沿的网络往往采用了众多的技术得到很好的效果。</p><p>这时候，作者团队希望能够化繁为简，探索一下哪些真正有用，哪些贡献不大。</p><h2 id="于是就有了-simsiam" tabindex="-1">于是就有了 SimSiam <a class="header-anchor" href="#于是就有了-simsiam" aria-label="Permalink to &quot;于是就有了 SimSiam&quot;">​</a></h2><p>是对前面几乎所有工作的总结，它提出了一个非常简单的网络，但是达到了很高的性能，并在其上追加了前面工作的一些细节，来展示每个小技术的真正贡献如何。</p><p>它不需要动量编码器，不需要负样本，不需要 memory bank，就是非常简单。</p><h1 id="模型结构" tabindex="-1">模型结构 <a class="header-anchor" href="#模型结构" aria-label="Permalink to &quot;模型结构&quot;">​</a></h1><p>模型的结构是一个 <code>“孪生网络”</code> ，其实于 BYOL 的结构很像，不过没有用动量编码器，左右两个编码器都是一样的，因此叫 <code>孪生网络</code> 。</p><p>虽然看起来只有左边预测右边，其实右边也有一个 predictor 去预测左边的特征，两边是对称的，左右的优化有先后顺序。</p><p><img src="https://cdn.xyxsw.site/boxcnWk5QzvbsSNlyV4B7SMt5zb.png" alt=""></p><p>结构其实没什么特殊的地方，主要讲讲思想。</p><h1 id="simsiam-主要回答的是两个问题" tabindex="-1">SimSiam 主要回答的是两个问题 <a class="header-anchor" href="#simsiam-主要回答的是两个问题" aria-label="Permalink to &quot;SimSiam 主要回答的是两个问题&quot;">​</a></h1><h1 id="_1-为什么不用负样本模型不会坍塌" tabindex="-1">1. 为什么不用负样本模型不会坍塌？ <a class="header-anchor" href="#_1-为什么不用负样本模型不会坍塌" aria-label="Permalink to &quot;1.为什么不用负样本模型不会坍塌？&quot;">​</a></h1><p>原论文中提出的解释并不是最完美的。而且这个问题的解释涉及了动力学等知识，我也没有足够的知识储备去讲解这个问题，这里只能讲一些与解答相关的信息，如果有兴趣可以看下面链接中的解释：</p><p>这里要涉及到一个机器学习的经典算法，<strong>EM 算法</strong>，它也是<strong> k-means</strong> 的核心思想之一。</p><p>因为本文的主旨原因，我不会在这里细讲这个算法，但是大家了解这是个什么东西即可。</p><p><strong>EM 算法</strong>用于优化带有未知参数的模型， <code>k-means</code> 的聚类中心就可以看作一个未知参数，我们要同时优化模型本体和聚类中心。所以我们先对其中一个目标 A 做<strong>随机初始化</strong>，然后<strong>先优化</strong>另一个目标 B，再反过来用另一个目标 B 优化后<strong>的结果优化被随机初始化的目标 A</strong>，这就是一次迭代，只要不断循环这个迭代，EM 算法往往能找到最优解。</p><p>这里可以把<strong>经过 predictor 预测头的特征</strong>作为 <code>k-means</code> 里的特征，而另一个作为<strong>目标的特征</strong>作为<strong>聚类中心</strong>，经过预测头的特征直接反向传播进行优化，作为目标的特征则是通过上面说的对称的操作经过预测头进行优化。</p><p>最最直白地解读结论的话，可以说是，这种先后优化的 EM 算法，使得模型 “来不及 “去把权重全部更新为 0。（模型坍塌）具体的推导需要动力学的知识，这里不做展开。</p><h1 id="_2-对前人工作的总结" tabindex="-1">2. 对前人工作的总结 <a class="header-anchor" href="#_2-对前人工作的总结" aria-label="Permalink to &quot;2.对前人工作的总结&quot;">​</a></h1><p>这是作者总结的所有” 孪生网络 “的模型结构，很精炼。</p><p><img src="https://cdn.xyxsw.site/boxcn8OWwnN8ae2vUVttqlu5O8e.png" alt=""></p><p>下面是这些网络训练结果的对比，也列出了它们分别有哪些 trick（用的是分类任务）</p><pre><code>                               负样本    动量编码器             训练轮数 
</code></pre><p><img src="https://cdn.xyxsw.site/boxcn3uizAKNhAxQryOwvHxFSDb.png" alt=""></p><p>具体结果还是图片比较直观（</p><p><img src="https://cdn.xyxsw.site/boxcnqdfrOIxim4wBayDDBitHCd.png" alt=""></p>`,29),s=[o];function n(p,c,m,d,h,l){return t(),e("div",null,s)}const g=a(r,[["render",n]]);export{S as __pageData,g as default};
