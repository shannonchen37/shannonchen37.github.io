import{_ as l,o as n,c as t,j as s,U as a}from"./chunks/framework.489e5108.js";const g=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"4.人工智能/ch02/ch2.2/ch2.2.5/2.2.5.0.md","filePath":"4.人工智能/ch02/ch2.2/ch2.2.5/2.2.5.0.md","lastUpdated":1696176798000}'),e={name:"4.人工智能/ch02/ch2.2/ch2.2.5/2.2.5.0.md"},o=a('<h2 id="背景与动机" tabindex="-1">背景与动机 <a class="header-anchor" href="#背景与动机" aria-label="Permalink to &quot;背景与动机&quot;">​</a></h2><p>在推荐系统的精排模块，多任务学习的模型结构已成业界的主流，获得了广阔的应用。多任务学习（multi-task learning），本质上是希望使用一个模型完成多个任务的建模。在推荐系统中，多任务学习一般即指多目标学习（multi-label learning），不同目标输入相同的 feature 进行联合训练，是迁移学习的一种。他们之间的关系如图：</p><div align="center"><img src="https://pic1.zhimg.com/80/v2-130d040474f34095ec6d8c81133da538_1440w.jpg" style="zoom:60%;"></div><p>下面我们先讨论三个问题</p><p><strong>一、为什么要用多任务学习？</strong></p><p>（1）很多业界推荐的业务，天然就是一个多目标的建模场景，需要多目标共同优化。以微信视频号推荐为例，打开一个视频，如图，首页上除了由于视频自动播放带来的 “播放时长”、“完播率”（用户播放时长占视频长度的比例）目标之外，还有大量的互动标签，例如 “点击好友头像”、“进入主页”、“关注”、“收藏”、“分享”、“点赞”、“评论” 等。究竟哪一个标签最符合推荐系统的建模目标呢？</p><div align="center"><img src="https://pic4.zhimg.com/80/v2-c18fc1ec65e308ee2e1477d7868007db_1440w.jpg" style="zoom:30%;"></div><p>如果要用一个词来概括所有各式各样的推荐系统的终极目标，那就是 “用户满意度”，但我们无法找到一个显示的指标量化用户满意度。业界一般使用 “DAU”、“用户日均使用时长”、“留存率” 来作为客观的间接的 “用户满意度”（或者说算法工程师绩效）评价指标。而这些指标都是难以通过单一目标建模的，以使用时长为例，长视频播放长度天然大于短视频。所幸的是，虽然没有显式的用户满意度评价指标，但是现在的 app 都存在类似上述视频号推荐场景的丰富具体的隐式反馈。但这些独立的隐式反馈也存在一些挑战：</p><ul><li>目标偏差：点赞、分享表达的满意度可能比播放要高</li><li>物品偏差：不同视频的播放时长体现的满意度不一样，有的视频可能哄骗用户看到尾部（类似新闻推荐中的标题党）</li><li>用户偏差：有的用户表达满意喜欢用点赞，有的用户可能喜欢用收藏</li></ul><p>因此我们需要使用多任务学习模型针对多个目标进行预测，并在线上融合多目标的预测结果进行排序。多任务学习也不能直接表达用户满意度，但是可以最大限度利用能得到的用户反馈信息进行充分的表征学习，并且可建模业务之间的关系，从而高效协同学习具体任务。</p><p>（2）工程便利，不用针对不同的任务训练不同的模型。一般推荐系统中排序模块延时需求在 40ms 左右，如果分别对每个任务单独训练一个模型，难以满足需求。出于控制成本的目的，需要将部分模型进行合并。合并之后，能更高效的利用训练资源和进行模型的迭代升级。</p><p><strong>二、为什么多任务学习有效？</strong></p><p>当把业务独立建模变成多任务联合建模之后，有可能带来四种结果：</p><div align="center"><img src="https://pic1.zhimg.com/80/v2-44927ccdd6caf9685d3d9d5367af98dc_1440w.jpg" style="zoom:60%;"></div><p>多任务学习的优势在于通过部分参数共享，联合训练，能在保证 “还不错” 的前提下，实现多目标共同提升。原因有以下几种：</p><ul><li>任务互助：对于某个任务难学到的特征，可通过其他任务学习</li><li>隐式数据增强：不同任务有不同的噪声，一起学习可抵消部分噪声</li><li>学到通用表达，提高泛化能力：模型学到的是对所有任务都偏好的权重，有助于推广到未来的新任务</li><li>正则化：对于一个任务而言，其他任务的学习对该任务有正则化效果</li></ul><p><strong>三、多任务学习都在研究什么问题</strong>？</p><p>如上所述，多任务的核心优势在于通过不同任务的网络参数共享，实现 1+1&gt;2 的提升，因此多任务学习的一大主流研究方向便是如何设计有效的网络结构。多个 label 的引入自然带来了多个 loss，那么如何在联合训练中共同优化多个 loss 则是关键问题。</p><ul><li>网络结构设计：主要研究哪些参数共享、在什么位置共享、如何共享。这一方向我们认为可以分为两大类，第一类是在设计网络结构时，考虑目标间的显式关系（例如淘宝中，点击之后才有购买行为发生），以阿里提出的 ESMM 为代表；另一类是目标间没有显示关系（例如短视频中的收藏与分享），在设计模型时不考虑 label 之间的量化关系，以谷歌提出的 MMOE 为代表。</li><li>多 loss 的优化策略：主要解决 loss 数值有大有小、学习速度有快有慢、更新方向时而相反的问题。最经典的两个工作有 UWL（Uncertainty Weight）：通过自动学习任务的 uncertainty，给 uncertainty 大的任务小权重，uncertainty 小的任务大权重；GradNorm：结合任务梯度的二范数和 loss 下降梯度，引入带权重的损失函数 Gradient Loss，并通过梯度下降更新该权重。</li></ul><h2 id="loss加权融合" tabindex="-1">loss 加权融合 <a class="header-anchor" href="#loss加权融合" aria-label="Permalink to &quot;loss加权融合&quot;">​</a></h2><p>一种最简单的实现多任务学习的方式是对不同任务的 loss 进行加权。例如谷歌的 Youtube DNN 论文中提到的一种加权交叉熵：</p>',21),p={class:"MathJax",jax:"SVG",display:"true",style:{direction:"ltr",display:"block","text-align":"center",margin:"1em 0",position:"relative"}},r={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-2.697ex"},xmlns:"http://www.w3.org/2000/svg",width:"60.196ex",height:"4.847ex",role:"img",focusable:"false",viewBox:"0 -950 26606.6 2142.2","aria-hidden":"true"},Q=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="A0" d="" style="stroke-width:3;"></path><path data-c="57" d="M792 683Q810 680 914 680Q991 680 1003 683H1009V637H996Q931 633 915 598Q912 591 863 438T766 135T716 -17Q711 -22 694 -22Q676 -22 673 -15Q671 -13 593 231L514 477L435 234Q416 174 391 92T358 -6T341 -22H331Q314 -21 310 -15Q309 -14 208 302T104 622Q98 632 87 633Q73 637 35 637H18V683H27Q69 681 154 681Q164 681 181 681T216 681T249 682T276 683H287H298V637H285Q213 637 213 620Q213 616 289 381L364 144L427 339Q490 535 492 546Q487 560 482 578T475 602T468 618T461 628T449 633T433 636T408 637H380V683H388Q397 680 508 680Q629 680 650 683H660V637H647Q576 637 576 619L727 146Q869 580 869 600Q869 605 863 612T839 627T794 637H783V683H792Z" transform="translate(250,0)" style="stroke-width:3;"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(1278,0)" style="stroke-width:3;"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(1722,0)" style="stroke-width:3;"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(2000,0)" style="stroke-width:3;"></path><path data-c="68" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 124T102 167T103 217T103 272T103 329Q103 366 103 407T103 482T102 542T102 586T102 603Q99 622 88 628T43 637H25V660Q25 683 27 683L37 684Q47 685 66 686T103 688Q120 689 140 690T170 693T181 694H184V367Q244 442 328 442Q451 442 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(2500,0)" style="stroke-width:3;"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(3056,0)" style="stroke-width:3;"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(3445,0)" style="stroke-width:3;"></path><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" transform="translate(3889,0)" style="stroke-width:3;"></path><path data-c="20" d="" transform="translate(4445,0)" style="stroke-width:3;"></path><path data-c="43" d="M56 342Q56 428 89 500T174 615T283 681T391 705Q394 705 400 705T408 704Q499 704 569 636L582 624L612 663Q639 700 643 704Q644 704 647 704T653 705H657Q660 705 666 699V419L660 413H626Q620 419 619 430Q610 512 571 572T476 651Q457 658 426 658Q322 658 252 588Q173 509 173 342Q173 221 211 151Q232 111 263 84T328 45T384 29T428 24Q517 24 571 93T626 244Q626 251 632 257H660L666 251V236Q661 133 590 56T403 -21Q262 -21 159 83T56 342Z" transform="translate(4695,0)" style="stroke-width:3;"></path><path data-c="45" d="M128 619Q121 626 117 628T101 631T58 634H25V680H597V676Q599 670 611 560T625 444V440H585V444Q584 447 582 465Q578 500 570 526T553 571T528 601T498 619T457 629T411 633T353 634Q266 634 251 633T233 622Q233 622 233 621Q232 619 232 497V376H286Q359 378 377 385Q413 401 416 469Q416 471 416 473V493H456V213H416V233Q415 268 408 288T383 317T349 328T297 330Q290 330 286 330H232V196V114Q232 57 237 52Q243 47 289 47H340H391Q428 47 452 50T505 62T552 92T584 146Q594 172 599 200T607 247T612 270V273H652V270Q651 267 632 137T610 3V0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(5417,0)" style="stroke-width:3;"></path><path data-c="20" d="" transform="translate(6098,0)" style="stroke-width:3;"></path><path data-c="4C" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q48 680 182 680Q324 680 348 683H360V637H333Q273 637 258 635T233 622L232 342V129Q232 57 237 52Q243 47 313 47Q384 47 410 53Q470 70 498 110T536 221Q536 226 537 238T540 261T542 272T562 273H582V268Q580 265 568 137T554 5V0H25V46H58Q100 47 109 49T128 61V622Z" transform="translate(6348,0)" style="stroke-width:3;"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(6973,0)" style="stroke-width:3;"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(7473,0)" style="stroke-width:3;"></path><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z" transform="translate(7867,0)" style="stroke-width:3;"></path><path data-c="A0" d="" transform="translate(8261,0)" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(8788.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(9844.6,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="munder" transform="translate(10789.2,0)"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(600,-1084.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mrow" transform="translate(12399.9,0)"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(278,0)"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(617,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="msub" transform="translate(1189,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(523,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mi" transform="translate(2172.6,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" style="stroke-width:3;"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)" style="stroke-width:3;"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3450.6,0)"><path data-c="2061" d="" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(3617.2,0)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(536,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mo" transform="translate(4669.4,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mrow" transform="translate(5669.6,0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(389,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1111.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(2111.4,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(523,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mo" transform="translate(2928.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mi" transform="translate(9153.7,0)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" style="stroke-width:3;"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(278,0)" style="stroke-width:3;"></path><path data-c="67" d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z" transform="translate(778,0)" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(10431.7,0)"><path data-c="2061" d="" style="stroke-width:3;"></path></g><g data-mml-node="mrow" transform="translate(10598.4,0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(389,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1111.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(2111.4,0)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(536,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mo" transform="translate(2941.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(13928.8,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z" style="stroke-width:3;"></path></g></g></g></g>',1),T=[Q],c=s("mjx-assistive-mml",{unselectable:"on",display:"block",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",overflow:"hidden",width:"100%"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("mtext",null," Weighted CE Loss "),s("mo",null,"="),s("mo",null,"−"),s("munder",null,[s("mo",{"data-mjx-texclass":"OP"},"∑"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mi",null,"i")])]),s("mrow",{"data-mjx-texclass":"INNER"},[s("mo",{"data-mjx-texclass":"OPEN"},"["),s("msub",null,[s("mi",null,"T"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mi",null,"i")])]),s("msub",null,[s("mi",null,"y"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mi",null,"i")])]),s("mi",null,"log"),s("mo",{"data-mjx-texclass":"NONE"},"⁡"),s("msub",null,[s("mi",null,"p"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mi",null,"i")])]),s("mo",null,"+"),s("mrow",{"data-mjx-texclass":"INNER"},[s("mo",{"data-mjx-texclass":"OPEN"},"("),s("mn",null,"1"),s("mo",null,"−"),s("msub",null,[s("mi",null,"y"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mi",null,"i")])]),s("mo",{"data-mjx-texclass":"CLOSE"},")")]),s("mi",null,"log"),s("mo",{"data-mjx-texclass":"NONE"},"⁡"),s("mrow",{"data-mjx-texclass":"INNER"},[s("mo",{"data-mjx-texclass":"OPEN"},"("),s("mn",null,"1"),s("mo",null,"−"),s("msub",null,[s("mi",null,"p"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mi",null,"i")])]),s("mo",{"data-mjx-texclass":"CLOSE"},")")]),s("mo",{"data-mjx-texclass":"CLOSE"},"]")])])],-1),d=a(`<p>其中<img src="https://www.zhihu.com/equation?tex=T_i" alt="[公式]"> 为观看时长。在原始训练数据中，正样本是视频展示后用户点击了该视频，负样本则是展示后未点击，这个一个标准的 CTR 预估问题。该 loss 通过改变训练样本的权重，让所有负样本的权重都为 1，而正样本的权重为点击后的视频观看时长 <img src="https://www.zhihu.com/equation?tex=T_i" alt="[公式]"> 。作者认为按点击率排序会倾向于把诱惑用户点击（用户未必真感兴趣) 的视频排前面，而观看时长能更好地反映出用户对视频的兴趣，通过重新设计 loss 使得该模型在保证主目标点击的同时，将视频观看时长转化为样本的权重，达到优化平均观看时长的效果。</p><p>另一种更为简单粗暴的加权方式是人工手动调整权重，例如 0.3*L (点击)+0.7*L*(视频完播)</p><p>这种 loss 加权的方式优点如下：</p><ul><li>模型简单，仅在训练时通过梯度乘以样本权重实现对其它目标的加权</li><li>模型上线简单，和 base 完全相同，不需要额外开销</li></ul><p>缺点：</p><ul><li>本质上并不是多目标建模，而是将不同的目标转化为同一个目标。样本的加权权重需要根据 AB 测试才能确定。</li></ul><h2 id="shared-bottom" tabindex="-1">Shared-Bottom <a class="header-anchor" href="#shared-bottom" aria-label="Permalink to &quot;Shared-Bottom&quot;">​</a></h2><p>最早的多任务学习模型是底层共享结构（Shared-Bottom），如图所示。</p><p>通过共享底层模块，学习任务间通用的特征表征，再往上针对每一个任务设置一个 Tower 网络，每个 Tower 网络的参数由自身对应的任务目标进行学习。Shared Bottom 可以根据自身数据特点，使用 MLP、DeepFM、DCN、DIN 等，Tower 网络一般使用简单的 MLP。</p><p>代码如下，共享特征 embedding，共享底层 DNN 网络，任务输出层独立，loss 直接使用多个任务的 loss 值之和。</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">Shared_Bottom</span><span style="color:#E1E4E8;">(dnn_feature_columns, num_tasks</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">, task_types</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">, task_names</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">                  bottom_dnn_units</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">[</span><span style="color:#79B8FF;">128</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">128</span><span style="color:#E1E4E8;">], tower_dnn_units_lists</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">[[</span><span style="color:#79B8FF;">64</span><span style="color:#E1E4E8;">,</span><span style="color:#79B8FF;">32</span><span style="color:#E1E4E8;">], [</span><span style="color:#79B8FF;">64</span><span style="color:#E1E4E8;">,</span><span style="color:#79B8FF;">32</span><span style="color:#E1E4E8;">]],</span></span>
<span class="line"><span style="color:#E1E4E8;">                  l2_reg_embedding</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0.00001</span><span style="color:#E1E4E8;">, l2_reg_dnn</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, seed</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">1024</span><span style="color:#E1E4E8;">,dnn_dropout</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">                  dnn_activation</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&#39;relu&#39;</span><span style="color:#E1E4E8;">, dnn_use_bn</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    features </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> build_input_features(dnn_feature_columns)</span></span>
<span class="line"><span style="color:#E1E4E8;">    inputs_list </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">list</span><span style="color:#E1E4E8;">(features.values())</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span></span>
<span class="line"><span style="color:#E1E4E8;">    sparse_embedding_list, dense_value_list </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> input_from_feature_columns(features, dnn_feature_columns, l2_reg_embedding,seed)</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#6A737D;">#共享输入特征</span></span>
<span class="line"><span style="color:#E1E4E8;">    dnn_input </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> combined_dnn_input(sparse_embedding_list, dense_value_list)</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#6A737D;">#共享底层网络</span></span>
<span class="line"><span style="color:#E1E4E8;">    shared_bottom_output </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> DNN(bottom_dnn_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, </span><span style="color:#FFAB70;">seed</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">seed)(dnn_input)</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#6A737D;">#任务输出层</span></span>
<span class="line"><span style="color:#E1E4E8;">    tasks_output </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> []</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> task_type, task_name, tower_dnn </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">zip</span><span style="color:#E1E4E8;">(task_types, task_names, tower_dnn_units_lists):</span></span>
<span class="line"><span style="color:#E1E4E8;">        tower_output </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> DNN(tower_dnn, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, </span><span style="color:#FFAB70;">seed</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">seed, </span><span style="color:#FFAB70;">name</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&#39;tower_&#39;</span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;">task_name)(shared_bottom_output)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">        logit </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tf.keras.layers.Dense(</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">use_bias</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">activation</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">)(tower_output)</span></span>
<span class="line"><span style="color:#E1E4E8;">        output </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> PredictionLayer(task_type, </span><span style="color:#FFAB70;">name</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">task_name)(logit) </span></span>
<span class="line"><span style="color:#E1E4E8;">        tasks_output.append(output)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    model </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tf.keras.models.Model(</span><span style="color:#FFAB70;">inputs</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">inputs_list, </span><span style="color:#FFAB70;">outputs</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">tasks_output)</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">return</span><span style="color:#E1E4E8;"> model</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">Shared_Bottom</span><span style="color:#24292E;">(dnn_feature_columns, num_tasks</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, task_types</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">None</span><span style="color:#24292E;">, task_names</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">None</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">                  bottom_dnn_units</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">[</span><span style="color:#005CC5;">128</span><span style="color:#24292E;">, </span><span style="color:#005CC5;">128</span><span style="color:#24292E;">], tower_dnn_units_lists</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">[[</span><span style="color:#005CC5;">64</span><span style="color:#24292E;">,</span><span style="color:#005CC5;">32</span><span style="color:#24292E;">], [</span><span style="color:#005CC5;">64</span><span style="color:#24292E;">,</span><span style="color:#005CC5;">32</span><span style="color:#24292E;">]],</span></span>
<span class="line"><span style="color:#24292E;">                  l2_reg_embedding</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">0.00001</span><span style="color:#24292E;">, l2_reg_dnn</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">, seed</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">1024</span><span style="color:#24292E;">,dnn_dropout</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">0</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">                  dnn_activation</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">&#39;relu&#39;</span><span style="color:#24292E;">, dnn_use_bn</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">):</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    features </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> build_input_features(dnn_feature_columns)</span></span>
<span class="line"><span style="color:#24292E;">    inputs_list </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">list</span><span style="color:#24292E;">(features.values())</span></span>
<span class="line"><span style="color:#24292E;">    </span></span>
<span class="line"><span style="color:#24292E;">    sparse_embedding_list, dense_value_list </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> input_from_feature_columns(features, dnn_feature_columns, l2_reg_embedding,seed)</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;">#共享输入特征</span></span>
<span class="line"><span style="color:#24292E;">    dnn_input </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> combined_dnn_input(sparse_embedding_list, dense_value_list)</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;">#共享底层网络</span></span>
<span class="line"><span style="color:#24292E;">    shared_bottom_output </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> DNN(bottom_dnn_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, </span><span style="color:#E36209;">seed</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">seed)(dnn_input)</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;">#任务输出层</span></span>
<span class="line"><span style="color:#24292E;">    tasks_output </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> []</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> task_type, task_name, tower_dnn </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">zip</span><span style="color:#24292E;">(task_types, task_names, tower_dnn_units_lists):</span></span>
<span class="line"><span style="color:#24292E;">        tower_output </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> DNN(tower_dnn, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, </span><span style="color:#E36209;">seed</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">seed, </span><span style="color:#E36209;">name</span><span style="color:#D73A49;">=</span><span style="color:#032F62;">&#39;tower_&#39;</span><span style="color:#D73A49;">+</span><span style="color:#24292E;">task_name)(shared_bottom_output)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        logit </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> tf.keras.layers.Dense(</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, </span><span style="color:#E36209;">use_bias</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">False</span><span style="color:#24292E;">, </span><span style="color:#E36209;">activation</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">None</span><span style="color:#24292E;">)(tower_output)</span></span>
<span class="line"><span style="color:#24292E;">        output </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> PredictionLayer(task_type, </span><span style="color:#E36209;">name</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">task_name)(logit) </span></span>
<span class="line"><span style="color:#24292E;">        tasks_output.append(output)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    model </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> tf.keras.models.Model(</span><span style="color:#E36209;">inputs</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">inputs_list, </span><span style="color:#E36209;">outputs</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">tasks_output)</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> model</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><p>优点：</p><ul><li>浅层参数共享，互相补充学习，任务相关性越高，模型 loss 优化效果越明显，也可以加速训练。</li></ul><p>缺点：</p><ul><li>任务不相关甚至优化目标相反时（例如新闻的点击与阅读时长），可能会带来负收益，多个任务性能一起下降。</li></ul><p>一般把 Shared-Bottom 的结构称作 “参数硬共享”，多任务学习网络结构设计的发展方向便是如何设计更灵活的共享机制，从而实现 “参数软共享”。</p><p>参考资料：</p><p><a href="https://developer.aliyun.com/article/793252" target="_blank" rel="noreferrer">https://developer.aliyun.com/article/793252</a></p><p><a href="https://zhuanlan.zhihu.com/p/291406172" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/291406172</a></p><p>Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks (ICML&#39;2018)</p><p>UWL: Multi-task learning using uncertainty to weigh losses for scene geometry and semantics (CVPR&#39;2018)</p><p>YoutubeDNN: Deep neural networks for youtube recommendations (RecSys&#39;2016)</p>`,22);function i(m,y,E,h,u,_){return n(),t("div",null,[o,s("mjx-container",p,[(n(),t("svg",r,T)),c]),d])}const V=l(e,[["render",i]]);export{g as __pageData,V as default};
