import{_ as a,o,c as t,U as e}from"./chunks/framework.489e5108.js";const b=JSON.parse('{"title":"MoCo v3","description":"","frontmatter":{},"headers":[],"relativePath":"4.人工智能/4.6.8.9MoCo v3.md","filePath":"4.人工智能/4.6.8.9MoCo v3.md","lastUpdated":1696176798000}'),r={name:"4.人工智能/4.6.8.9MoCo v3.md"},s=e('<h1 id="moco-v3" tabindex="-1">MoCo v3 <a class="header-anchor" href="#moco-v3" aria-label="Permalink to &quot;MoCo v3&quot;">​</a></h1><h1 id="前言" tabindex="-1">前言 <a class="header-anchor" href="#前言" aria-label="Permalink to &quot;前言&quot;">​</a></h1><p>在 VIT 出来之后，大家也都想着把对比学习的 backbone 换成 VIT。于是新的缝合怪自然而然地诞生了</p><p>MoCo v3，它缝合了 MoCo 和 SimSiam，以及新的骨干网络 VIT。</p><h1 id="模型架构" tabindex="-1">模型架构 <a class="header-anchor" href="#模型架构" aria-label="Permalink to &quot;模型架构&quot;">​</a></h1><p>可能因为和前面的工作太像了，作者就没有给模型总览图，我们借 MoCo 的总览图来讲</p><p><img src="https://cdn.xyxsw.site/boxcnhxg4HZw2NExIbYZxQGISze.png" alt=""></p><p>总体架构其实没有太多变化，还是 memory bank 的结构，右边也还是动量编码器，不过加入了 SimCLR 提出的 projection head（就是额外的那层 mlp），并且在对比上用了 SimSiam 的预测头对称学习方式。具体也不展开了，都是老东西缝合在一起。</p><h1 id="讲这篇文章主要是冲着-vit-来的" tabindex="-1">讲这篇文章主要是冲着 VIT 来的 <a class="header-anchor" href="#讲这篇文章主要是冲着-vit-来的" aria-label="Permalink to &quot;讲这篇文章主要是冲着 VIT 来的&quot;">​</a></h1><p>作者在用 VIT 做骨干网络训练的时候，发现如下问题：</p><p><img src="https://cdn.xyxsw.site/boxcnMMhbVk6wc81H8BSoack7Mg.png" alt=""></p><p>在使用 VIT 训练的时候，batchsize 不算太大时训练很平滑，但是一旦 batchsize 变大，训练的图像就会出现如上图这样的<strong>波动</strong>。于是作者去查看了每一层的梯度，发现问题出在<strong> VIT 的第一层线性变换</strong>上。也就是下图中的粉色那个层，<strong>把图片打成 patch 后展平做的线性变换</strong>。</p><p><img src="https://cdn.xyxsw.site/boxcniBkiypcv6IQbxr9D6JukOb.png" alt=""></p><p>在这一层中，梯度会出现波峰，而正确率则会突然下跌。</p><p>作者就想，既然你训练不好，我就把你冻住，不让它训练，然后就能成功运用了。而且这个 trick 对几乎所有的对比学习模型都有效果。</p>',15),i=[s];function n(c,p,h,_,d,l){return o(),t("div",null,i)}const x=a(r,[["render",n]]);export{b as __pageData,x as default};
